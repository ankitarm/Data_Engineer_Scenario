{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdXtq7huPTfP7GRmDCkVs2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitarm/Data_Engineer_Scenario/blob/main/Data_Engineer_Mocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mock 1 : https://www.youtube.com/watch?v=-iMhSpP77F8"
      ],
      "metadata": {
        "id": "nQpEwDg0Yrb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. Please explain what Kinesis is and its role.\n",
        "### **What is AWS Kinesis?**  \n",
        "AWS Kinesis is a **fully managed, scalable, real-time data streaming service** provided by Amazon Web Services (AWS). It is designed to **collect, process, and analyze streaming data** (e.g., logs, metrics, transactions, social media feeds) in real time.  \n",
        "\n",
        "Kinesis is often compared to **Apache Kafka** (an open-source distributed event streaming platform), but unlike Kafka, Kinesis is **fully managed by AWS**, eliminating the need to handle cluster setup, scaling, or maintenance.\n",
        "\n",
        "---\n",
        "\n",
        " **Key Components of Kinesis**  \n",
        "Kinesis consists of multiple services, each serving different streaming needs:  \n",
        "\n",
        "1. **Kinesis Data Streams**  \n",
        "   - **Role:** Ingests and stores real-time data records (e.g., clickstreams, IoT sensor data).  \n",
        "\n",
        "2. **Kinesis Data Firehose**  \n",
        "   - **Role:** **Automatically loads streaming data into AWS destinations** (S3, Redshift, Elasticsearch, etc.).  \n"
      ],
      "metadata": {
        "id": "vYptpEUvY7eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. Can you elaborate more on diff between kafka and kinesis\n",
        "\n",
        "\n",
        "| **Feature**               | **Apache Kafka**                                   | **AWS Kinesis**                                  |\n",
        "|---------------------------|---------------------------------------------------|-------------------------------------------------|\n",
        "| **Managed Service**       | Self-hosted (requires setup/maintenance)          | Fully managed by AWS                            |\n",
        "| **Scalability**           | Manual scaling (add brokers/partitions)           | Auto-scaling (adjust shards dynamically)        |\n",
        "| **Latency**               | Ultra-low (~10ms)                                 | Higher (~200ms)                                 |\n",
        "| **Durability**            | Configurable replication (user-managed)           | Built-in replication across AZs                 |\n",
        "| **Pricing Model**         | Open-source (infrastructure costs apply)          | Pay per shard/hour + data volume               |\n",
        "| **Data Retention**         | Configurable (days to years)                      | Fixed (1–365 days)                              |\n",
        "| **Throughput**            | Higher (supports 100k+ msgs/sec per partition)    | Lower (1MB/sec or 1k msgs/sec per shard)       |\n",
        "| **Partitions/Shards**     | Partitions (user-managed)                         | Shards (AWS-managed)                            |\n",
        "| **Integrations**          | Multi-cloud, on-prem, hybrid                      | AWS-native (Lambda, S3, Redshift, etc.)        |\n",
        "| **Use Cases**             | High-throughput, low-latency, cross-platform apps | Real-time analytics, AWS-centric pipelines     |\n",
        "| **Setup Complexity**      | High (requires tuning, monitoring)                | Low (serverless, minimal configuration)        |\n",
        "| **Security**              | Custom (SSL, SASL, Kerberos)                      | AWS IAM, KMS encryption                        |\n",
        "| **Consumer Model**        | Pull-based (consumers request data)               | Push/pull hybrid (Kinesis Client Library)      |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways:**\n",
        "1. **Choose Kafka** if you need:  \n",
        "   - Cross-platform deployment (e.g., hybrid cloud).  \n",
        "   - Ultra-low latency (e.g., financial trading).  \n",
        "   - Full control over infrastructure.  \n",
        "\n",
        "2. **Choose Kinesis** if you:  \n",
        "   - Prefer a serverless, AWS-native solution.  \n",
        "   - Need quick setup without managing clusters.  \n",
        "   - Use AWS services (e.g., Lambda, Firehose).  \n",
        "\n",
        "Which to Choose?\n",
        "Need long retention (years) or flexibility? → Kafka.\n",
        "\n",
        "Want simplicity and AWS integration? → Kinesis (but max 1 year)."
      ],
      "metadata": {
        "id": "GbNVrdbXdpum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. **Kafka vs. Kinesis: Producer-Consumer or Pub/Sub?**  \n",
        "\n",
        "\n",
        "| **Model Aspect**       | **Apache Kafka**                                   | **AWS Kinesis**                                  |\n",
        "|------------------------|---------------------------------------------------|-------------------------------------------------|\n",
        "| **Primary Model**      | **Pub/Sub** (with consumer groups)                | **Producer-Consumer** (with shard-level parallelism) |\n",
        "| **Data Flow**          | - Producers → **Topics** → Consumers (pull-based) | - Producers → **Streams/Shards** → Consumers (push/pull hybrid) |\n",
        "| **Subscription Style** | - Consumers **subscribe to topics** (flexible)    | - Consumers **read from shards** (fixed mapping) |\n",
        "| **Message Delivery**   | - **Pull-based** (consumers request data)         | - **Push (KCL) or Pull (Lambda, SDKs)**         |\n",
        "| **Parallelism**        | - **Partitions** allow parallel consumers         | - **Shards** enable parallel processing         |\n",
        "| **Decoupling**         | - High (multiple consumer groups per topic)       | - Medium (consumers compete for shard leases)   |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences**  \n",
        "1. **Kafka = Pub/Sub with Flexibility**  \n",
        "   - Producers publish to **topics**, and **consumer groups** subscribe.  \n",
        "   - Multiple independent consumer groups can read the same data (e.g., one for analytics, another for alerts).  \n",
        "   - Example:  \n",
        "     ```python\n",
        "     # Kafka Pub/Sub Example\n",
        "     producer.send(\"orders-topic\", order_data)  # Publisher\n",
        "     consumer.subscribe([\"orders-topic\"])      # Subscriber\n",
        "     ```\n",
        "\n",
        "2. **Kinesis = Producer-Consumer with Scalability**  \n",
        "   - Producers write to **shards**, and consumers **lease shards** for processing.  \n",
        "   - Each shard supports **one consumer at a time** (no native fan-out).  \n",
        "   - Example:  \n",
        "     ```python\n",
        "     # Kinesis Producer-Consumer Example\n",
        "     kinesis.put_record(StreamName=\"logs-stream\", Data=log_data)  # Producer\n",
        "     shard_reader = kinesis.get_shard_iterator(...)               # Consumer\n",
        "     ```\n",
        "\n"
      ],
      "metadata": {
        "id": "dfix4F_Le-mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. What is the difference between producer-consumer and publisher-subscriber?\n",
        "\n",
        "### **Producer-Consumer vs. Publisher-Subscriber**  \n",
        "\n",
        "These are two fundamental messaging patterns in distributed systems, differing in **coupling**, **scalability**, and **message delivery** mechanics.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Producer-Consumer Model**  \n",
        "**Definition:**  \n",
        "- A **synchronous, point-to-point** messaging pattern.  \n",
        "- **Producers** send messages to a **queue**, and **consumers** pull messages from it.  \n",
        "- Each message is processed by **exactly one consumer**.  \n",
        "\n",
        "**Key Traits:**  \n",
        "✅ **Point-to-Point (1:1)** – Only one consumer gets each message.  \n",
        "✅ **Tight Coupling** – Producers/consumers must know the queue.  \n",
        "✅ **Pull-Based** – Consumers request messages when ready.  \n",
        "✅ **Order Guaranteed** – FIFO (First-In-First-Out) by default.  \n",
        "\n",
        "**Example:**  \n",
        "- A **task queue** (e.g., RabbitMQ, SQS).  \n",
        "- Workers processing orders from an e-commerce system.  \n",
        "\n",
        "**Diagram:**  \n",
        "```\n",
        "Producer → [ Queue ] → Consumer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Publisher-Subscriber (Pub/Sub) Model**  \n",
        "**Definition:**  \n",
        "- An **asynchronous, broadcast** messaging pattern.  \n",
        "- **Publishers** send messages to a **topic**, and **subscribers** receive them.  \n",
        "- Each message is processed by **all interested subscribers**.  \n",
        "\n",
        "**Key Traits:**  \n",
        "✅ **Broadcast (1:N)** – Multiple subscribers receive the same message.  \n",
        "✅ **Loose Coupling** – Publishers/subscribers only know the topic, not each other.  \n",
        "✅ **Push-Based** – Messages are pushed to subscribers (or pulled via polling).  \n",
        "✅ **Dynamic Scaling** – Subscribers can join/leave anytime.  \n",
        "\n",
        "**Example:**  \n",
        "- **Stock market feeds** (multiple apps listen to price updates).  \n",
        "- **Event-driven microservices** (e.g., Kafka, AWS SNS).  \n",
        "\n",
        "**Diagram:**  \n",
        "```\n",
        "Publisher → [ Topic ] → Subscriber 1  \n",
        "                     → Subscriber 2  \n",
        "                     → Subscriber 3\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Differences Summary**  \n",
        "| **Aspect**          | **Producer-Consumer**                | **Publisher-Subscriber**             |\n",
        "|----------------------|--------------------------------------|--------------------------------------|\n",
        "| **Messaging Style**  | Point-to-point (1:1)                 | Broadcast (1:N)                      |\n",
        "| **Coupling**         | Tight (queue-aware)                  | Loose (topic-based)                  |\n",
        "| **Delivery**         | Pull-based (consumer-driven)         | Push-based (or polled)               |\n",
        "| **Scalability**      | Limited (competing consumers)        | High (independent subscribers)       |\n",
        "| **Use Cases**        | Task queues, ordered processing      | Real-time notifications, event buses |\n",
        "\n",
        "---\n",
        "\n",
        "### **Which to Choose?**  \n",
        "- **Use Producer-Consumer** when:  \n",
        "  - You need **exactly-once processing** (e.g., order fulfillment).  \n",
        "  - Order matters (e.g., FIFO queues).  \n",
        "\n",
        "- **Use Pub/Sub** when:  \n",
        "  - Multiple systems need **the same data** (e.g., analytics + alerts).  \n",
        "  - You want **decoupled, event-driven architectures**.  \n",
        "\n",
        "**Hybrid Systems (Kafka/Kinesis):**  \n",
        "- Both models can coexist (e.g., Kafka uses **consumer groups** for Pub/Sub-like behavior while maintaining ordering per partition).  \n",
        "\n",
        "Would you like a real-world analogy (e.g., postal service vs. radio broadcast)?"
      ],
      "metadata": {
        "id": "t4kLAuOyh0Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6. Why are Kafka or Kinesis used instead of simple messaging queues?\n",
        "\n",
        "### **Message Ordering in Kafka vs. Kinesis**  \n",
        "\n",
        "#### **✅ Kafka**  \n",
        "- **Ordered per partition**: Messages in the same partition are strictly ordered (FIFO).  \n",
        "- **No global order**: Across partitions, order is **not** guaranteed unless using a single partition (bottleneck).  \n",
        "- **Key-based routing**: Same key → same partition → preserves order for related messages.  \n",
        "- **Use case**: E.g., ensuring all events for a user ID are processed in sequence.  \n",
        "\n",
        "#### **✅ Kinesis**  \n",
        "- **Ordered per shard**: Messages in the same shard are strictly ordered.  \n",
        "- **No cross-shard order**: Like Kafka, order is lost across shards.  \n",
        "- **Partition key routing**: Same key → same shard → ordered sequence.  \n",
        "- **Use case**: E.g., processing financial transactions in exact arrival order per account.  \n",
        "\n",
        "#### **⚠️ Key Limitation**  \n",
        "- Both **cannot guarantee global order** (across partitions/shard) without sacrificing scalability.  \n"
      ],
      "metadata": {
        "id": "QzuR7OQMj-lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7. Can messages be ordered in Kafka or Kinesis?\n",
        "\n",
        "### **Message Ordering in Kafka vs. Kinesis**  \n",
        "\n",
        "**Short Answer:**  \n",
        "Yes, but **only within a partition (Kafka) or shard (Kinesis)**. Global ordering across all partitions/shard is **not guaranteed**.  \n",
        "\n",
        "#### **Kafka**  \n",
        "- **Order Guarantee**: Strictly ordered **per partition**.  \n",
        "  - Example: If messages M1, M2, M3 are sent to **Partition 1**, they’ll be consumed in order.  \n",
        "  - Cross-partition order is **not guaranteed** (e.g., M1 in Partition 1 and M4 in Partition 2 may arrive out of order).  \n",
        "- **How to Preserve Order**:  \n",
        "  - Use a **message key** (e.g., `user_id`) to ensure related messages go to the same partition.  \n",
        "\n",
        "#### **Kinesis**  \n",
        "- **Order Guarantee**: Strictly ordered **per shard**.  \n",
        "  - Example: Messages in **Shard A** are ordered, but Shard A vs. Shard B may deliver data out of sequence.  \n",
        "- **How to Preserve Order**:  \n",
        "  - Use a **partition key** (e.g., `device_id`) to route related messages to the same shard.  \n",
        "\n",
        "---\n",
        "\n",
        "### **When Order Matters**  \n",
        "- **Use Cases**:  \n",
        "  - **Kafka**: Financial transactions (e.g., debit before credit).  \n",
        "  - **Kinesis**: Clickstream sequences (e.g., page A → page B).  \n",
        "- **Trade-off**:  \n",
        "  - Ordering reduces parallelism (since keys must map to the same partition/shard).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Interview Tip**  \n",
        "*\"Kafka and Kinesis guarantee ordering only within a partition/shard. For global ordering, you’d need a single partition (which limits throughput) or external sequencing (like a ledger).\"*  \n",
        "\n"
      ],
      "metadata": {
        "id": "_CIJdDQ7lQGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8. **Priority Queue: Definition & Key Concepts**  \n",
        "A **priority queue** is a specialized data structure where elements are processed based on **priority** (not just insertion order).  \n",
        "\n",
        "#### **Key Characteristics (Bullet Points)**  \n",
        "- **Not FIFO**: Unlike regular queues, the **highest-priority** element is removed first.  \n",
        "- **Priority Assignment**: Each element has an associated priority (e.g., numerical value, urgency level).  \n",
        "- **Operations**:  \n",
        "  - **Insert**: Add an element with a priority.  \n",
        "  - **Extract**: Remove the highest/lowest priority element.  \n",
        "- **Implementation**: Typically uses a **heap** (binary heap) for efficient O(log n) inserts/extracts.  \n",
        "\n",
        "#### **Real-World Examples**  \n",
        "- **Hospital ER**: Critical patients (high priority) are treated before mild cases.  \n",
        "- **CPU Scheduling**: High-priority tasks (e.g., system processes) run before background apps.  \n",
        "- **Ride-Sharing**: Premium users get faster driver allocation.  \n",
        "\n",
        "\n",
        "#### **Example in Python**  \n",
        "```python\n",
        "import heapq\n",
        "\n",
        "pq = []\n",
        "heapq.heappush(pq, (2, \"Task A\"))  # (priority, data)\n",
        "heapq.heappush(pq, (1, \"Task B\"))  # Lower number = higher priority\n",
        "print(heapq.heappop(pq)[1])         # Output: \"Task B\" (higher priority)\n",
        "```\n",
        "\n",
        "#### **Interview Tip**  \n",
        "- Mention trade-offs: **Heaps** (O(log n)) vs. **sorted lists** (O(n) insert).  \n",
        "- Use cases where order matters beyond FIFO (e.g., real-time systems).  \n",
        "\n"
      ],
      "metadata": {
        "id": "L2_k8cJMn-71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q9. **Process vs. Thread (Short Interview Answer)**  \n",
        "\n",
        "- **Process**:  \n",
        "  - Independent program instance with **separate memory** (isolated).  \n",
        "  - Heavyweight (slower to create/switch).  \n",
        "  - Crash in one process **doesn’t affect others**.  \n",
        "\n",
        "- **Thread**:  \n",
        "  - Lightweight **subset of a process**, shares memory/resources.  \n",
        "  - Faster to create/switch (less overhead).  \n",
        "  - Crash in one thread **can crash the entire process**.  \n",
        "\n",
        "**Key Difference**:  \n",
        "- Processes = **Isolated execution** (e.g., separate browser tabs).  \n",
        "- Threads = **Parallel tasks within a process** (e.g., rendering + downloads in one tab).  \n"
      ],
      "metadata": {
        "id": "cO2NI6TYo1mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10. **CAP Theorem: Short & Clear Explanation**  \n",
        "\n",
        "**CAP Theorem** states that in a distributed system, you can only guarantee **two out of three** properties at the same time:  \n",
        "\n",
        "1. **Consistency (C)**  \n",
        "   - All nodes see the **same data at the same time** (e.g., a bank balance is accurate across all servers).  \n",
        "\n",
        "2. **Availability (A)**  \n",
        "   - Every request gets a **response (even if stale)**, with no system downtime.  \n",
        "\n",
        "3. **Partition Tolerance (P)**  \n",
        "   - The system keeps working **even if nodes disconnect** (e.g., network failure).  \n",
        "\n",
        "#### **Trade-offs (Pick 2/3)**  \n",
        "- **CA** (e.g., SQL databases): Sacrifice partition tolerance (single-server systems).  \n",
        "- **CP** (e.g., MongoDB, Kafka): Sacrifice availability (e.g., reject requests if nodes can’t sync).  \n",
        "- **AP** (e.g., Cassandra, DynamoDB): Sacrifice consistency (allow stale reads during partitions).  \n",
        "\n",
        "### **Real-World Examples of CAP Theorem Trade-offs**  \n",
        "\n",
        "#### **1. CP (Consistency + Partition Tolerance)**  \n",
        "**Example: Banks & Financial Systems**  \n",
        "- **Why?** Transactions **must** be consistent (e.g., $100 withdrawn can’t show $90 in one server and $100 in another).  \n",
        "- **Sacrifice:** Availability (e.g., during a network partition, the system may reject transactions to avoid inconsistencies).  \n",
        "- **Tech:** PostgreSQL, Kafka (for transactions), Zookeeper.  \n",
        "\n",
        "#### **2. AP (Availability + Partition Tolerance)**  \n",
        "**Example: Social Media (Facebook, Twitter)**  \n",
        "- **Why?** Better to show **stale data** (e.g., delayed like counts) than fail to load the page.  \n",
        "- **Sacrifice:** Consistency (temporary mismatches are acceptable).  \n",
        "- **Tech:** Cassandra, DynamoDB, Redis (eventual consistency).  \n",
        "\n",
        "#### **3. CA (Consistency + Availability)**  \n",
        "**Example: Single-Node Databases (Rare in Distributed Systems)**  \n",
        "- **Why?** No partitions (single server), so it’s always consistent and available.  \n",
        "- **Sacrifice:** Partition tolerance (fails if the network splits).  \n",
        "- **Tech:** SQLite, standalone MySQL (non-replicated).  \n",
        "\n",
        "### **Key Insight**  \n",
        "- **Most distributed systems choose **CP or AP** (since partitions are inevitable).  \n",
        "- **CA systems** are rare in modern cloud architectures (they’re not fault-tolerant).  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uYIcHd-vsJO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q11. SQL Query to Find Percentage of Users Who Bought AirPods After iPhones\n",
        "\n",
        "To solve this problem, we need to:\n",
        "1. Identify users who bought an iPhone\n",
        "2. Check if their next purchase was AirPods\n",
        "3. Calculate the percentage of such users out of all iPhone buyers\n",
        "\n",
        "Here's the SQL solution:\n",
        "\n",
        "```sql\n",
        "WITH UserPurchaseSequence AS (\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        product,\n",
        "        transaction_timestamp,\n",
        "        LEAD(product) OVER (PARTITION BY customer_id ORDER BY transaction_timestamp) AS next_product\n",
        "    FROM transactions\n",
        "),\n",
        "\n",
        "iPhoneBuyers AS (\n",
        "    SELECT DISTINCT customer_id\n",
        "    FROM transactions\n",
        "    WHERE product = 'iPhone'\n",
        "),\n",
        "\n",
        "iPhoneToAirPodsBuyers AS (\n",
        "    SELECT DISTINCT customer_id\n",
        "    FROM UserPurchaseSequence\n",
        "    WHERE product = 'iPhone' AND next_product = 'AirPods'\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    (COUNT(DISTINCT iab.customer_id) * 100.0 /\n",
        "    NULLIF(COUNT(DISTINCT ib.customer_id), 0) AS percentage\n",
        "FROM iPhoneBuyers ib\n",
        "LEFT JOIN iPhoneToAirPodsBuyers iab ON ib.customer_id = iab.customer_id;\n",
        "```\n",
        "\n",
        "## Explanation:\n",
        "\n",
        "1. **UserPurchaseSequence CTE**:\n",
        "   - Uses the `LEAD()` window function to see what each customer bought next\n",
        "   - Partitions by customer and orders by timestamp to get chronological sequence\n",
        "\n",
        "2. **iPhoneBuyers CTE**:\n",
        "   - Identifies all unique customers who bought iPhones (denominator)\n",
        "\n",
        "3. **iPhoneToAirPodsBuyers CTE**:\n",
        "   - Finds customers whose immediate next purchase after iPhone was AirPods (numerator)\n",
        "\n",
        "4. **Final Calculation**:\n",
        "   - Divides the count of customers who bought AirPods after iPhones by total iPhone buyers\n",
        "   - Multiplies by 100 to get percentage\n",
        "   - Uses NULLIF to avoid division by zero\n"
      ],
      "metadata": {
        "id": "q2vfJS3Qtf0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q12. Will the code work if you try to delete dictionary items during iteration?\n",
        "\n",
        "### **Deleting Dictionary Items During Iteration in Python**\n",
        "\n",
        "**Short Answer:**  \n",
        "**No**, the code will **not** work correctly if you try to delete dictionary items during iteration. It raises a **`RuntimeError: dictionary changed size during iteration`**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why It Fails**\n",
        "Python dictionaries track their size, and modifying them (adding/removing keys) while iterating breaks the internal iterator, causing an immediate error.\n",
        "\n",
        "#### **Example of the Problem:**\n",
        "```python\n",
        "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
        "\n",
        "for key in my_dict:\n",
        "    if my_dict[key] % 2 == 0:\n",
        "        del my_dict[key]  # 🚨 RuntimeError\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Fix It**\n",
        "#### **Option 1: Iterate Over a Copy of Keys**\n",
        "```python\n",
        "for key in list(my_dict.keys()):  # Explicit copy\n",
        "    if my_dict[key] % 2 == 0:\n",
        "        del my_dict[key]  # Safe\n",
        "```\n",
        "\n",
        "#### **Option 2: Store Keys to Delete First**\n",
        "```python\n",
        "keys_to_delete = [key for key, value in my_dict.items() if value % 2 == 0]\n",
        "for key in keys_to_delete:\n",
        "    del my_dict[key]\n",
        "```\n",
        "\n",
        "#### **Option 3: Use Dictionary Comprehension (Creates New Dict)**\n",
        "```python\n",
        "my_dict = {k: v for k, v in my_dict.items() if v % 2 != 0}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "- **Never modify a dict while iterating directly** over it.  \n",
        "- **Safe methods**:  \n",
        "  - Iterate over a **copy** of keys (`list(my_dict.keys())`).  \n",
        "  - Record keys to delete first, then delete them.  \n",
        "  - Use **dictionary comprehension** for filtering.  \n",
        "\n"
      ],
      "metadata": {
        "id": "ljUv9bIEturq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
        "for key in list(my_dict.keys()):  # Explicit copy\n",
        "    if my_dict[key] % 2 == 0:\n",
        "        del my_dict[key]\n",
        "print( my_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX0hejRCu380",
        "outputId": "2fa82991-ac4f-4504-84db-97b17c27f455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'c': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q13. What is the difference between Pandas and PySpark?\n",
        "### **Pandas vs. PySpark: Key Differences**\n",
        "\n",
        "| **Feature**               | **Pandas**                                  | **PySpark**                                  |\n",
        "|---------------------------|---------------------------------------------|---------------------------------------------|\n",
        "| **Execution Environment** | Single-machine (RAM-limited)                | Distributed (cluster of machines)           |\n",
        "| **Scalability**           | Handles data that fits in memory (~GBs)     | Handles TBs+ via parallel processing       |\n",
        "| **Lazy Evaluation**       | No (immediate execution)                    | Yes (optimizes execution plan)              |\n",
        "| **Fault Tolerance**       | No (crash = data loss)                      | Yes (via RDD lineage recovery)             |\n",
        "| **Syntax**                | Pythonic (similar to NumPy)                 | SQL-like (with DataFrame API)               |\n",
        "| **Use Cases**             | EDA, small datasets, single-node workflows  | Big data pipelines, ETL, cloud processing  |\n",
        "| **Performance**           | Faster for small data (low overhead)        | Slower for tiny data (cluster setup cost)  |\n",
        "| **Integration**           | Works with Python libraries (Matplotlib, etc.) | Integrates with Hadoop/Hive/S3/etc.       |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Which?**\n",
        "- **Choose Pandas when:**\n",
        "  - Data fits in memory (e.g., <10GB).\n",
        "  - You need quick prototyping or EDA.\n",
        "  - Your workflow uses Python ML libraries (scikit-learn, TensorFlow).\n",
        "\n",
        "- **Choose PySpark when:**\n",
        "  - Data exceeds memory (e.g., TBs of logs).\n",
        "  - You need distributed processing (e.g., ETL pipelines).\n",
        "  - Working in cloud environments (AWS EMR, Databricks).\n",
        "\n",
        "---\n",
        "\n",
        "### **Code Comparison**\n",
        "#### **Pandas (Single-Node)**\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df_filtered = df[df[\"value\"] > 100]  # In-memory operation\n",
        "```\n",
        "\n",
        "#### **PySpark (Distributed)**\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df = spark.read.csv(\"s3://bucket/data.csv\")\n",
        "df_filtered = df.filter(df[\"value\"] > 100)  # Lazy evaluation\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Technical Differences**\n",
        "1. **Memory vs. Disk**:  \n",
        "   - Pandas operates in memory.  \n",
        "   - PySpark spills to disk when needed.  \n",
        "\n",
        "2. **Parallelism**:  \n",
        "   - Pandas: Single-threaded (unless using `modin.pandas`).  \n",
        "   - PySpark: Automatically partitions data across nodes.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "h4bMXa4IuBuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q14. **Step-by-Step Approach to Migrate from Pandas to PySpark**  \n",
        "\n",
        "#### **1. Assess the Current Pipeline**  \n",
        "- **Identify bottlenecks**: Check RAM/CPU usage, slow transformations, and I/O operations.  \n",
        "- **Document dependencies**: Note Pandas functions used (e.g., `.apply()`, `.groupby()`).  \n",
        "- **Data volume**: Confirm if data size justifies PySpark (e.g., >10GB or growing).  \n",
        "\n",
        "#### **2. Set Up PySpark Environment**  \n",
        "- **Cluster configuration**: Choose # of nodes/cores based on data size (e.g., AWS EMR, Databricks).  \n",
        "- **Dependencies**: Install `pyspark` and ensure compatibility with Python libraries (e.g., `numpy`, `scikit-learn`).  \n",
        "\n",
        "#### **3. Rewrite Code Logic**  \n",
        "- **Replace Pandas functions with PySpark equivalents**:  \n",
        "\n",
        "  | **Pandas**               | **PySpark**                          |  \n",
        "  |--------------------------|--------------------------------------|  \n",
        "  | `df[df.col > 100]`       | `df.filter(df.col > 100)`            |  \n",
        "  | `df.groupby().agg()`     | `df.groupBy().agg()` (*case-sensitive*) |  \n",
        "  | `df.apply(func)`         | `df.withColumn(\"new\", udf(func))`    |  \n",
        "  | `pd.merge()`             | `df1.join(df2, on=\"key\")`            |  \n",
        "\n",
        "- **Handle lazy evaluation**: Call `.cache()` for reused DataFrames and `.collect()` only when needed.  \n",
        "\n",
        "#### **4. Optimize Performance**  \n",
        "- **Partitioning**: Repartition data to avoid skew (e.g., `df.repartition(100)`).  \n",
        "- **Broadcast small DataFrames**: Use `broadcast()` for joins with small tables.  \n",
        "- **Avoid UDFs**: Prefer built-in PySpark functions (e.g., `F.expr()` over Python UDFs).  \n",
        "\n",
        "#### **5. Test and Validate**  \n",
        "- **Unit tests**: Verify logic matches Pandas output (e.g., `assert df_pandas.equals(df_spark.toPandas())`).  \n",
        "- **Scale testing**: Run on sample data → full dataset to catch memory/shuffle issues.  \n",
        "- **Data integrity checks**: Compare row counts, NULL values, and key metrics.  \n",
        "\n",
        "#### **6. Deploy and Monitor**  \n",
        "- **Orchestration**: Schedule jobs via Airflow/Luigi (for batch) or Spark Streaming (for real-time).  \n",
        "- **Monitoring**: Track Spark UI for slow tasks, skew, or spills to disk.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example Migration**  \n",
        "**Pandas**:  \n",
        "```python  \n",
        "df = pd.read_csv(\"data.csv\")  \n",
        "df[\"discount\"] = df[\"price\"].apply(lambda x: x * 0.9)  \n",
        "```  \n",
        "\n",
        "**PySpark**:  \n",
        "```python  \n",
        "from pyspark.sql import functions as F  \n",
        "df = spark.read.csv(\"data.csv\")  \n",
        "df = df.withColumn(\"discount\", F.col(\"price\") * 0.9)  # Avoid UDF for speed  \n",
        "```  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mVFcGSEDwkh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mock 2:https://www.youtube.com/watch?v=E01qDvTjRic"
      ],
      "metadata": {
        "id": "6ZAIJeKRzySB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ABfOHAGU7DuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "YElR8Msl7Ffm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. Difference between RDD, Dataframe and Dataset.\n",
        "\n",
        "> \"All three — RDD, DataFrame, and Dataset — are abstractions provided by Apache Spark for working with distributed data. They differ mainly in **level of abstraction**, **performance optimization**, and **type safety**.\"\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ RDD (Resilient Distributed Dataset)\n",
        "\n",
        "> \"RDD is the **most fundamental** and **low-level** abstraction in Spark.\n",
        "\n",
        "It gives **fine-grained control** but **lacks built-in optimization**. There’s **no schema**, no catalyst optimizer, and you don’t get the benefits of Spark's engine-level tuning.\n",
        "\n",
        "👉 **Use RDD when**:\n",
        "\n",
        "* You need **fine control over data processing**\n",
        "* Working with **unstructured data**\n",
        "* Doing low-level transformations or legacy code\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ DataFrame\n",
        "\n",
        "> \"DataFrame is a **higher-level abstraction** introduced in Spark 1.3. It’s like a distributed version of a SQL table, with rows and named columns. It’s backed by Spark SQL and **optimized by the Catalyst optimizer** which makes it **much faster** than RDDs.\n",
        "\n",
        "\n",
        "\n",
        "👉 **Use DataFrame when**:\n",
        "\n",
        "* You want **performance + ease of use**\n",
        "* You're doing **ETL, reporting, or analytics**\n",
        "* Schema is known and structured\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Dataset (Scala/Java Only)\n",
        "\n",
        "> \"Dataset sits in between RDD and DataFrame and combines the **benefits of both**. It’s **type-safe** like RDD and **optimized** like DataFrame.\n",
        "\n",
        "Dataset is mainly used in **Scala and Java**, not Python (because Python lacks compile-time type safety).\"\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Interview Summary Line\n",
        "\n",
        "> \"Use **RDD** when you need low-level control and can trade off performance.\n",
        "> Use **DataFrame** when you want better performance and simplicity with structured data.\n",
        "> Use **Dataset** in Scala/Java when you need both **type safety** and **query optimization**.\"\n"
      ],
      "metadata": {
        "id": "qkunU0Tv7Glx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. Different compression techniques such as snappy, biz2 and LZO.\n",
        "\n",
        "> \"In big data systems, compression helps reduce **storage costs** and **I/O time** — especially when dealing with petabytes of data. But not all compression algorithms are equal — some prioritize speed, while others focus on reducing size. Choosing the right one depends on the use case.\"\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **Snappy**\n",
        "\n",
        "> \"Snappy is a compression algorithm developed by Google. It’s optimized for **speed** rather than maximum compression ratio. It’s widely used in Hadoop, Spark, and data lakes because it offers a good **tradeoff between compression and decompression speed**.\"\n",
        "\n",
        "* **Fast compression and decompression**\n",
        "* **Block-level compression** (great for columnar formats like Parquet)\n",
        "* Ideal for **real-time and interactive workloads** (e.g., Spark jobs)\n",
        "* Native support in **Parquet, ORC, Avro**\n",
        "\n",
        "👉 Use Snappy when **performance and speed** are more important than storage savings.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **Bzip2**\n",
        "\n",
        "> \"Bzip2 focuses on **high compression ratio**, which means it compresses more aggressively but at the cost of **much slower speed**. It’s good when you're archiving data or trying to minimize space, but it's not suitable for interactive jobs.\"\n",
        "\n",
        "* **Higher compression ratio** (smaller file size)\n",
        "* **Slower** compression and decompression\n",
        "* Not splittable by default — bad for Hadoop unless indexed\n",
        "* Often used for **long-term storage or backups**\n",
        "\n",
        "👉 Use Bzip2 when **space savings** matter more than speed (e.g., archival storage).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **LZO**\n",
        "\n",
        "> \"LZO is similar to Snappy in terms of **speed** — it’s also designed for **fast compression and decompression**, but with slightly better compression than Snappy in some cases. It’s splittable, which is useful in Hadoop ecosystems.\"\n",
        "\n",
        "* **Fast like Snappy**\n",
        "* Slightly better compression than Snappy, but less common\n",
        "* **Splittable** — good for distributed processing (unlike Bzip2)\n",
        "* Requires **extra setup** in Hadoop compared to Snappy\n",
        "\n",
        "👉 Use LZO if you want speed **and** you need your files to be **splittable** for parallel processing in Hadoop.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Interview Summary Answer\n",
        "\n",
        "> \"Snappy and LZO are best for **fast processing and parallel computation**, making them ideal for Spark, Hive, or real-time pipelines. Bzip2 offers **better compression ratio** but is slow and not splittable, so it’s more suited for **cold storage or archival**.\n",
        "\n",
        "In most production data pipelines, we use **Snappy with Parquet or ORC** formats because it's fast, lightweight, and fully supported across distributed systems.\"\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZlUvfYbg7XRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. What is the write-ahead log?\n",
        "\n",
        "> \"A Write-Ahead Log (WAL) is a technique used in databases and distributed systems to ensure **data durability and crash recovery**.\n",
        "> The core idea is: **Before applying any change to the actual data, the change is first recorded in a log.** This way, even if the system crashes during the update, it can replay the log and recover to a consistent state.\"\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 How It Works (Simple Steps)\n",
        "\n",
        "1. A write request comes in.\n",
        "2. The system **writes the change to the WAL (sequentially)**.\n",
        "3. Only after the log is written, the **actual data is updated**.\n",
        "4. If the system crashes, the WAL is used to **replay or roll back** operations.\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 Where It’s Used\n",
        "\n",
        "* **Relational databases** (e.g., PostgreSQL, MySQL InnoDB)\n",
        "* **Distributed systems** (e.g., Apache Kafka, Apache Flink)\n",
        "* **Big data tools** (e.g., Delta Lake, Apache HBase)\n",
        "* **File systems** (e.g., HDFS Namenode)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Key Benefits\n",
        "\n",
        "* ✅ **Durability**: Guarantees that committed transactions are not lost\n",
        "* ✅ **Crash recovery**: Replay the WAL to restore consistent state\n",
        "* ✅ **Performance**: Log writes are sequential, so they’re fast\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 🔥 Interview Soundbite\n",
        "\n",
        "> \"**Write-Ahead Logging** ensures durability in the face of failure. By recording every change in a log before applying it to the main data store, the system can always recover from crashes. It's a fundamental technique used in databases, file systems, and stream processing engines.\"\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_lYIFQrt7trh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. Difference between SparkSession & SparkContext.\n",
        "\n",
        "> \"`SparkContext` is the **entry point to low-level Spark APIs**, mainly for working with RDDs.\n",
        "\n",
        "`SparkSession` is the **unified entry point for all Spark functionalities** — including DataFrame, Dataset, and SQL operations. I\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> \"**Use `SparkSession` — it's the newer, more powerful API that wraps `SparkContext` and gives access to Spark SQL, DataFrames, and more.**\"\n"
      ],
      "metadata": {
        "id": "W_xk_FWw71Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. What are SQS and SNS in AWS?\n",
        "\n",
        "> **\"SQS (Simple Queue Service)** is a **message queue** used to decouple and buffer communication between distributed systems. It ensures reliable, asynchronous message delivery.\n",
        "\n",
        "> **SNS (Simple Notification Service)** is a **pub/sub messaging system** that pushes messages to multiple subscribers like SQS queues, Lambda functions, or email endpoints in real time.\n",
        "\n",
        "In short:\n",
        "**SQS = pull-based queue** (decouples producers and consumers)\n",
        "**SNS = push-based pub/sub** (broadcasts to multiple subscribers)\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HUWCirFO78CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6. What is the work of Step functions in aws?\n",
        "\n",
        "\n",
        "> \"**Step Functions** is a fully managed **state machine service** on AWS for orchestrating serverless workflows,\n",
        "> whereas **Apache Airflow** is an open-source **workflow scheduler** focused on **data pipelines** with more flexibility and extensibility.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **When to Use Each**\n",
        "\n",
        "* **AWS Step Functions**:\n",
        "\n",
        "  * Best for **event-driven**, **serverless**, and tightly integrated **AWS workflows**\n",
        "  * Great for orchestrating **Lambda, ECS, Batch, SageMaker**, etc.\n",
        "  * No infrastructure to manage; scales automatically\n",
        "  * Visual interface with built-in **retry, timeout, and error handling**\n",
        "  * Declarative (JSON/YAML-based) — limited logic control\n",
        "\n",
        "* **Apache Airflow**:\n",
        "\n",
        "  * Best for **complex data workflows**, **ETL pipelines**, or anything requiring **custom Python logic**\n",
        "  * Highly **extensible** via Python operators/plugins\n",
        "  * Can run **any task** (not just AWS)\n",
        "  * Requires infrastructure setup and maintenance (can be managed via MWAA or Astronomer)\n",
        "  * Fine-grained control over **scheduling**, **dependencies**, and **dynamic DAGs**\n",
        "\n",
        "---\n",
        "\n",
        "> \"**Step Functions** are ideal for orchestrating AWS-native, serverless workflows with minimal setup.\n",
        "> **Airflow** offers more flexibility and power for building complex, cross-platform **data pipelines**, but needs more management.\n",
        "> I’d choose Step Functions for quick AWS integrations and Airflow when I need rich DAG logic or hybrid workflows.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "5lAEGacu8I7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7. How do you decide which should go to the Data Warehouse and which should be treated as an external table?\n",
        "\n",
        "\n",
        "> \"If the data is **critical for analytics, reporting, or BI**, and needs to be **clean, structured, and performant**, it should go into the **Data Warehouse**.\n",
        "> If the data is **raw, semi-structured, very large, or infrequently queried**, it’s better to keep it as an **external table**—usually on data lake storage like S3 or HDFS—so it's cheaper to store and more flexible to process.\"\n",
        "\n",
        "Excellent follow-up. Here's how you can explain what an **external table** means in the context of the previous question — especially for an interview.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 What Is an External Table?\n",
        "\n",
        "> \"An **external table** is a table definition that points to **data stored outside the data warehouse**, typically in a data lake or file system like **Amazon S3, HDFS, or Azure Data Lake**.\n",
        "> The table only stores the **schema and metadata**, but not the data itself.\"\n",
        "\n",
        "In other words:\n",
        "\n",
        "* You're **not importing** the data into the warehouse\n",
        "* You're **querying it where it already lives** (e.g., Parquet/CSV files in S3)\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 Example (e.g., Hive, Athena, BigQuery External Table)\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL TABLE clickstream_logs (\n",
        "  user_id STRING,\n",
        "  timestamp TIMESTAMP,\n",
        "  page_url STRING\n",
        ")\n",
        "STORED AS PARQUET\n",
        "LOCATION 's3://my-bucket/logs/';\n",
        "```\n",
        "\n",
        "* This **defines a schema**, but doesn’t move the data.\n",
        "* Querying this table reads directly from the files in S3.\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Key Traits of External Tables\n",
        "\n",
        "* **Data remains in external storage**\n",
        "* Defined using **metadata only**\n",
        "* Often **used for raw or semi-structured data**\n",
        "* **Schema-on-read** (applied at query time, not load time)\n",
        "* Can be dropped **without deleting the underlying data**\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Interview Summary Answer\n",
        "\n",
        "> \"An **external table** allows you to query data where it lives — in your data lake — without moving it into the warehouse. It's useful when dealing with large volumes of raw, semi-structured, or infrequently used data. You define the schema in your SQL engine, but the data stays outside.\"\n"
      ],
      "metadata": {
        "id": "fuRkWave8L-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8. How do you choose the database in your project - relational and non-relational?\n",
        "\n",
        "**\"I choose a relational database when my data is structured, requires ACID transactions, strong consistency, and complex joins—like in financial or transactional systems.**\n",
        "\n",
        "**I prefer a non-relational (NoSQL) database when I need to handle unstructured or semi-structured data, require high scalability, flexible schemas, or need fast reads/writes—such as in caching, real-time analytics, or storing documents or key-value data.\"**\n"
      ],
      "metadata": {
        "id": "vJofFxqp-WAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q9. What is elastic search\n",
        "Elasticsearch is a powerful search engine that helps you quickly find and analyze large amounts of data, like text documents or logs. It’s built to be fast, scalable, and easy to use for searching through data in real-time."
      ],
      "metadata": {
        "id": "TwyfqLTT-jzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10. Difference between windows and group by.\n",
        "\n",
        "> \"`GROUP BY` is used when you want to aggregate rows – for example, calculating the total sales per customer. But it collapses rows – you lose individual detail.\n",
        "\n",
        "Window functions, on the other hand, allow you to perform calculations **across a set of rows** that are related to the current row, **without collapsing** the data. They’re great for running totals, ranking, percentiles, and time-based comparisons.\n",
        "\n",
        "So, if I want to know each employee’s salary and their **rank within their department**, I’d use a window function. If I only want the **total salary per department**, I’d use `GROUP BY`.\"\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ When to Use Each\n",
        "\n",
        "| Use Case                               | Use                                       |\n",
        "| -------------------------------------- | ----------------------------------------- |\n",
        "| Total sales per product                | `GROUP BY product_id`                     |\n",
        "| Each sale with total sales per product | `SUM(...) OVER (PARTITION BY product_id)` |\n",
        "| Customer with max purchase             | `GROUP BY + MAX()`                        |\n",
        "| Every purchase + rank of customer      | `RANK() OVER (PARTITION BY ...)`          |\n",
        "| Monthly sales trend with lag           | `LAG()/LEAD()` with `OVER(...)`           |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Summary Answer for Interview\n",
        "\n",
        "> \"Use `GROUP BY` when you want to summarize or aggregate data into fewer rows.\n",
        "> Use **window functions** when you need aggregate-like operations but still want to **retain the individual row details**.\n",
        "> Window functions are more powerful for **analytics and time-series use cases**, while `GROUP BY` is best for **summary reports**.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "rIS2Zsaa_Dh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q11. Difference between having and where.\n",
        "\n",
        "> “`WHERE` is more efficient because it reduces the number of rows before aggregation. `HAVING` should only be used when filtering aggregated results.”\n",
        "\n",
        "---\n",
        "\n",
        "> \"`WHERE` filters raw rows before any aggregation or grouping happens. `HAVING` is used to filter results **after** aggregation, such as filtering groups based on `SUM` or `COUNT`. For performance reasons, we should always use `WHERE` when possible to reduce the dataset early.\"\n",
        "\n",
        "\n",
        "\n",
        "## 🔍 **1. Conceptual Difference**\n",
        "\n",
        "| Aspect              | `WHERE` Clause                      | `HAVING` Clause                           |\n",
        "| ------------------- | ----------------------------------- | ----------------------------------------- |\n",
        "| Applies to          | **Rows** before grouping            | **Groups** after aggregation (`GROUP BY`) |\n",
        "| Filters             | Individual rows                     | Aggregated results                        |\n",
        "| Used with           | `SELECT`, `UPDATE`, `DELETE`        | Usually with `GROUP BY`                   |\n",
        "| Aggregate functions | ❌ Cannot use (`SUM`, `COUNT`, etc.) | ✅ Can use                                 |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ **2. Execution Order (Simplified)**\n",
        "\n",
        "SQL Query Execution Order (conceptual):\n",
        "\n",
        "```sql\n",
        "FROM → WHERE → GROUP BY → HAVING → SELECT → ORDER BY\n",
        "```\n",
        "\n",
        "So:\n",
        "\n",
        "* `WHERE` filters **before grouping**.\n",
        "* `HAVING` filters **after aggregation**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 **3. Example**\n",
        "\n",
        "Let’s say we have a `sales` table:\n",
        "\n",
        "| id | customer | region | amount |\n",
        "| -- | -------- | ------ | ------ |\n",
        "| 1  | Alice    | East   | 100    |\n",
        "| 2  | Bob      | West   | 200    |\n",
        "| 3  | Alice    | East   | 300    |\n",
        "| 4  | Bob      | West   | 150    |\n",
        "| 5  | Eve      | East   | 120    |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 Example with `WHERE`:\n",
        "\n",
        "**Question**: Get total sales per customer, but only for rows where `region = 'East'`.\n",
        "\n",
        "```sql\n",
        "SELECT customer, SUM(amount) AS total_sales\n",
        "FROM sales\n",
        "WHERE region = 'East'\n",
        "GROUP BY customer;\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "\n",
        "* `WHERE` filters out non-East rows first.\n",
        "* Then `GROUP BY` and `SUM` are applied.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 Example with `HAVING`:\n",
        "\n",
        "**Question**: Get total sales per customer, but **only show** customers with **total sales > 200**.\n",
        "\n",
        "```sql\n",
        "SELECT customer, SUM(amount) AS total_sales\n",
        "FROM sales\n",
        "GROUP BY customer\n",
        "HAVING SUM(amount) > 200;\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "\n",
        "* All rows are grouped first.\n",
        "* `HAVING` filters **after** the aggregation.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 Combined Example:\n",
        "\n",
        "```sql\n",
        "SELECT customer, SUM(amount) AS total_sales\n",
        "FROM sales\n",
        "WHERE region = 'East'\n",
        "GROUP BY customer\n",
        "HAVING SUM(amount) > 200;\n",
        "```\n",
        "\n",
        "This filters only \"East\" rows, groups by customer, and returns only those with > 200 sales in East.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ **4. Performance Difference**\n",
        "\n",
        "| Factor          | `WHERE`                           | `HAVING`                             |\n",
        "| --------------- | --------------------------------- | ------------------------------------ |\n",
        "| Execution Time  | **Faster** (early filtering)      | **Slower** (post-aggregation)        |\n",
        "| Optimization    | Better optimized by SQL engines   | Less optimized unless indexed fields |\n",
        "| Recommended Use | Always filter early using `WHERE` | Use `HAVING` **only when needed**    |\n"
      ],
      "metadata": {
        "id": "Sy5imxG-_PHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q12. Difference between groupby and reduceby.\n",
        "\n",
        "### 🧠 **How to Explain to an Interviewer**\n",
        "\n",
        "> \"Both `groupByKey` and `reduceByKey` are used to perform aggregations by key in Spark, but they differ significantly in performance and use case.\n",
        "\n",
        "* `groupByKey` groups all the values with the same key into a single list—*without doing any aggregation*. This causes a **full shuffle** of data, moving all values across the network, which is expensive and can lead to **out-of-memory** errors.\n",
        "\n",
        "* `reduceByKey`, on the other hand, performs a **local reduction (or combine)** on each partition *before* shuffling. This means **less data is moved**, and aggregation is more efficient.\n",
        "\n",
        "That’s why **reduceByKey is preferred** when performing aggregation, like sum, count, or average.\n",
        "Only use `groupByKey` when you need **all the raw values** for a key, like for custom sorting or full downstream processing.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Example\n",
        "\n",
        "```python\n",
        "# Dataset: [(k, v), (k, v), ...]\n",
        "data = sc.parallelize([\n",
        "    (\"a\", 1),\n",
        "    (\"b\", 2),\n",
        "    (\"a\", 3),\n",
        "    (\"b\", 4)\n",
        "])\n",
        "\n",
        "# groupByKey (collects all values)\n",
        "grouped = data.groupByKey()\n",
        "# Output: (\"a\", [1, 3]), (\"b\", [2, 4])\n",
        "\n",
        "# reduceByKey (aggregates during shuffle)\n",
        "reduced = data.reduceByKey(lambda x, y: x + y)\n",
        "# Output: (\"a\", 4), (\"b\", 6)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 Rule of Thumb\n",
        "\n",
        "> **\"Use `reduceByKey` when you can aggregate. Use `groupByKey` only if you absolutely need all values per key.\"**\n"
      ],
      "metadata": {
        "id": "BpZMVc_d_WHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q13. System Design and Data Model: Messaging system\n",
        "\n",
        "We are given a conversation where a system design and data modeling problem is discussed. The problem is about designing a system to monitor employee communications across multiple platforms (like Slack, Zoom, Microsoft Teams, email) for potential fraud or harassment. The goal is to create a product (data pipeline) that extracts data from these sources, processes it, and stores it in a way that allows for easy access by legal teams or others when needed. The system should handle both batch and streaming data.\n",
        "Key requirements:\n",
        "1. **Multiple Sources**: Slack, Zoom, Microsoft Teams, email (Outlook), etc.\n",
        "2. **Data Volume**: Large volume of data (chat logs, emails).\n",
        "3. **Processing**: Basic transformations to filter for unethical content (harassment, fraud) and prepare data for generative AI (like creating embeddings for similarity search).\n",
        "4. **Storage**: Data should be stored in different zones (raw, transformed) for consumption by data scientists or legal teams.\n",
        "5. **Access**: The system must allow exporting data securely to external parties (like lawyers).\n",
        "The conversation also covers data modeling and the overall system design, including:\n",
        "- Using cloud services (AWS, GCP, etc.)\n",
        "- Data extraction methods (APIs, CDC, Kafka, Kinesis)\n",
        "- Data storage (S3 for raw, Parquet for processed data)\n",
        "- Data processing (Spark, AWS Glue)\n",
        "- Data modeling (star schema with fact and dimension tables)\n",
        "- Data exposure (APIs, secure sharing, CSV exports)\n",
        "Now, let's break down the questions and provide the best answers.\n",
        "### I. System Design Questions and Best Answers\n",
        "1. **How would you design the data ingestion for multiple sources (Slack, Teams, Zoom, email) that may use different protocols (APIs, CDC, etc.)?**\n",
        "   - **Answer**:\n",
        "     - Use a combination of batch and streaming ingestion.\n",
        "     - For streaming sources (e.g., Slack messages in real-time), use Kafka or AWS Kinesis to capture data and land it in a raw data store (like S3).\n",
        "     - For batch sources (e.g., daily email dumps), use scheduled jobs (Airflow, AWS Glue) to extract via APIs and store in S3.\n",
        "     - For email (Outlook), use Microsoft Graph API to pull emails in batch or set up a streaming connector if possible.\n",
        "     - Ensure idempotency and handle duplicates.\n",
        "2. **How would you structure the storage (data lake) for this system?**\n",
        "   - **Answer**:\n",
        "     - Use a medallion architecture:\n",
        "       - **Bronze/Raw Zone**: Store raw data in S3 in the original format (JSON, CSV, etc.). Partition by date/source for efficient querying.\n",
        "       - **Silver/Cleansed Zone**: Convert data to Parquet format. Apply basic cleansing (e.g., deduplication, schema enforcement). Partition by date/source.\n",
        "       - **Gold/Curated Zone**: Store modeled data (fact and dimension tables in Parquet/ORC) for consumption. This is where we apply business logic (e.g., flagging unethical messages).\n",
        "3. **How would you process the data to detect unethical content?**\n",
        "   - **Answer**:\n",
        "     - Use Spark (AWS Glue, EMR) for batch processing and Spark Streaming for real-time.\n",
        "     - Implement transformation logic:\n",
        "       - Tokenize text and match against a list of unethical keywords (e.g., from a company's code of conduct).\n",
        "       - Use NLP techniques (e.g., sentiment analysis, entity recognition) for advanced detection.\n",
        "       - For generative AI preparation, create embeddings (using models like BERT) and store them in a vector database (e.g., FAISS, Pinecone) for similarity search.\n",
        "     - Output: A fact table with message details and a flag for unethical content.\n",
        "4. **How would you model the data for analysis?**\n",
        "   - **Answer**:\n",
        "     - Use a star schema:\n",
        "       - **Fact Table**: `message_fact`\n",
        "         - message_id (PK), sender_id (FK), receiver_id (FK), text_id (FK), date_id (FK), time_id (FK), channel_id (FK), is_unethical (boolean)\n",
        "       - **Dimension Tables**:\n",
        "         - `employee_dim`: employee_id (PK), name, email, department, etc.\n",
        "         - `text_dim`: text_id (PK), message_text, language, etc. (for text analysis)\n",
        "         - `date_dim`: date_id (PK), date, day, month, year, etc.\n",
        "         - `time_dim`: time_id (PK), hour, minute, etc.\n",
        "         - `channel_dim`: channel_id (PK), platform_name (e.g., Slack), channel_name\n",
        "     - This allows efficient filtering (e.g., \"show all unethical messages by employee X in June\").\n",
        "5. **How would you handle data quality and orchestration?**\n",
        "   - **Answer**:\n",
        "     - **Orchestration**: Use Airflow or AWS Step Functions to manage workflows (ingestion, processing, validation).\n",
        "     - **Data Quality**:\n",
        "       - At ingestion: Validate schema, non-null keys.\n",
        "       - After transformations: Check for duplicates, nulls, and business rules (e.g., unethical flag rate).\n",
        "       - Tools: Great Expectations, AWS Deequ, or custom Spark checks.\n",
        "6. **How would you expose the data to external parties (e.g., lawyers) securely?**\n",
        "   - **Answer**:\n",
        "     - **Internal Consumption**: Use a data warehouse (Redshift, Snowflake) for internal users. Create views for filtered data (e.g., by employee, date).\n",
        "     - **External Sharing**:\n",
        "       - Option 1: Generate encrypted CSV/Parquet files and share via a secure link (AWS S3 pre-signed URLs) with expiration. Notify via email.\n",
        "       - Option 2: Use Snowflake's data sharing feature (if using Snowflake) to share a read-only view with the external party.\n",
        "       - Option 3: Build a secure API (using API Gateway and Lambda) that returns filtered data. Authenticate via API keys (rotated regularly) and use TLS encryption.\n",
        "       - Security: Ensure data is encrypted in transit (TLS) and at rest (AES-256). Mask PII if needed.\n",
        "### II. Data Modeling Questions and Best Answers\n",
        "1. **What are the key entities and relationships?**\n",
        "   - **Answer**:\n",
        "     - Entities: Employee, Message, Text, Date, Time, Channel.\n",
        "     - Relationships:\n",
        "       - A Message is sent by one Employee (sender) and received by one Employee (receiver).\n",
        "       - A Message has one Text content.\n",
        "       - A Message is sent on one Date and Time.\n",
        "       - A Message is sent via one Channel.\n",
        "2. **How would you design the `text_dim` table to support text analysis?**\n",
        "   - **Answer**:\n",
        "     - `text_dim` table:\n",
        "       - text_id (PK)\n",
        "       - raw_text (original text)\n",
        "       - cleaned_text (after removing special characters, etc.)\n",
        "       - language\n",
        "       - embedding_vector (for generative AI, stored as an array of floats)\n",
        "     - This allows efficient text search and similarity matching.\n",
        "3. **Should we store the unethical flag in the fact or dimension table?**\n",
        "   - **Answer**: Store it in the fact table (`message_fact.is_unethical`). This is a property of the message event, not the text itself (the same text could be ethical in one context and unethical in another).\n",
        "4. **How would you handle slowly changing dimensions (SCD) for employee data?**\n",
        "   - **Answer**:\n",
        "     - Use SCD Type 2 for `employee_dim` to track changes (e.g., department changes). Add columns:\n",
        "       - start_date, end_date, is_current\n",
        "     - Example: If an employee moves from Sales to Marketing, insert a new row with the new department and update the old row's end_date.\n",
        "5. **How would you optimize the fact table for large-scale querying?**\n",
        "   - **Answer**:\n",
        "     - Partition by date_id and channel_id.\n",
        "     - Use columnar storage (Parquet/ORC) for compression and efficient scans.\n",
        "     - Cluster by sender_id and is_unethical for common filters.\n",
        "### Summary of Best Practices\n",
        "- **Extraction**: Use Kafka/Kinesis for streaming, batch APIs for others.\n",
        "- **Storage**: S3 with medallion architecture (bronze, silver, gold).\n",
        "- **Processing**: Spark for batch/streaming, with NLP for unethical content detection.\n",
        "- **Modeling**: Star schema with fact and dimension tables.\n",
        "- **Exposure**: Secure APIs, pre-signed URLs, or data sharing features.\n",
        "- **Security**: Encryption (TLS, AES-256), API keys, and access controls.\n",
        "This design meets scalability, security, and functional requirements."
      ],
      "metadata": {
        "id": "qWRW6Y1cNhJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q14. What libraries have you used in Python in Data engineering.\n",
        "\n",
        "1. **Data Ingestion**\n",
        "\n",
        "   * Read data from various sources:\n",
        "\n",
        "     ```python\n",
        "     pd.read_csv(), pd.read_json(), pd.read_sql(), pd.read_parquet(), pd.read_excel()\n",
        "     ```\n",
        "   * Example:\n",
        "\n",
        "     ```python\n",
        "     df = pd.read_csv(\"sales.csv\")\n",
        "     ```\n",
        "\n",
        "2. **Data Cleaning & Transformation**\n",
        "\n",
        "   * Handle missing values: `df.dropna()`, `df.fillna()`\n",
        "   * Rename columns: `df.rename()`\n",
        "   * Type conversion: `df.astype()`\n",
        "   * Replace values: `df.replace()`\n",
        "   * Filtering & slicing: `df[df['col'] > 10]`\n",
        "   * Example:\n",
        "\n",
        "     ```python\n",
        "     df['date'] = pd.to_datetime(df['date'])\n",
        "     df = df.dropna(subset=['price'])\n",
        "     ```\n",
        "\n",
        "3. **ETL (Extract, Transform, Load)**\n",
        "\n",
        "   * Perform joins, merges: `pd.merge()`, `df.join()`\n",
        "   * Grouping & aggregation: `df.groupby().agg()`\n",
        "   * Sorting, deduplication: `df.sort_values()`, `df.drop_duplicates()`\n",
        "\n",
        "4. **Exploratory Data Analysis (EDA)**\n",
        "\n",
        "   * Summary stats: `df.describe()`, `df.info()`\n",
        "   * Correlation matrix: `df.corr()`\n",
        "   * Value counts: `df['category'].value_counts()`\n",
        "\n",
        "5. **Exporting Data**\n",
        "\n",
        "   * Save to file or database:\n",
        "\n",
        "     ```python\n",
        "     df.to_csv(), df.to_parquet(), df.to_sql()\n",
        "     ```\n",
        "\n",
        "6. **Interfacing with Databases**\n",
        "\n",
        "   * Read/write using `SQLAlchemy` or `sqlite3`:\n",
        "\n",
        "     ```python\n",
        "     from sqlalchemy import create_engine\n",
        "     engine = create_engine('sqlite:///mydb.sqlite')\n",
        "     df.to_sql('table_name', engine)\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Limitations of Pandas in Data Engineering\n",
        "\n",
        "* **Memory-bound**: Not suitable for large datasets that don’t fit into memory.\n",
        "* **Not distributed**: Single-machine only; use **Dask**, **PySpark**, or **Polars** for larger datasets.\n",
        "\n",
        "Here’s a compact guide to **NumPy for Data Engineering**, with practical code examples that are commonly useful in data pipelines, ETL, and preprocessing tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### Numpy\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Create arrays\n",
        "arr = np.array([1, 2, 3, 4])\n",
        "matrix = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# Create arrays filled with zeros, ones, or random numbers\n",
        "zeros = np.zeros((3, 3))\n",
        "ones = np.ones((2, 2))\n",
        "rand = np.random.rand(2, 3)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 2. **Basic Array Operations**\n",
        "\n",
        "```python\n",
        "arr = np.array([10, 20, 30, 40])\n",
        "\n",
        "# Element-wise operations\n",
        "arr2 = arr * 2         # [20, 40, 60, 80]\n",
        "arr3 = arr + 5         # [15, 25, 35, 45]\n",
        "\n",
        "# Aggregate operations\n",
        "mean_val = np.mean(arr)\n",
        "sum_val = np.sum(arr)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 4. **Reshaping and Merging**\n",
        "\n",
        "```python\n",
        "a = np.array([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Reshape\n",
        "reshaped = a.reshape((2, 3))\n",
        "\n",
        "# Concatenate\n",
        "b = np.array([7, 8, 9])\n",
        "concat = np.concatenate((a, b))\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-hl_1BvZyaP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q15. Different functions in Spark\n",
        "\n",
        "## 🔹 1. **Reading and Writing Data**\n",
        "\n",
        "```python\n",
        "# Reading\n",
        "spark.read.csv(\"path\", header=True, inferSchema=True)\n",
        "spark.read.json(\"path\")\n",
        "spark.read.parquet(\"path\")\n",
        "spark.read.orc(\"path\")\n",
        "spark.read.format(\"jdbc\").options(...).load()\n",
        "\n",
        "# Writing\n",
        "df.write.csv(\"path\", mode=\"overwrite\")\n",
        "df.write.parquet(\"path\")\n",
        "df.write.json(\"path\")\n",
        "df.write.mode(\"append\").saveAsTable(\"table_name\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2. **DataFrame Transformations**\n",
        "\n",
        "```python\n",
        "df.select(\"col1\", \"col2\")                   # select columns\n",
        "df.filter(df[\"col\"] > 100)                  # filter rows\n",
        "df.withColumn(\"new_col\", df[\"col\"] + 1)     # add/modify column\n",
        "df.drop(\"col_to_drop\")                      # drop column\n",
        "df.distinct()                               # remove duplicates\n",
        "df.dropDuplicates([\"col1\", \"col2\"])         # remove based on columns\n",
        "df.limit(10)                                # limit rows\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 3. **Aggregations and Grouping**\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df.groupBy(\"col\").agg(F.sum(\"amount\"), F.avg(\"score\"))\n",
        "df.agg(F.max(\"col\"), F.min(\"col\"))\n",
        "df.count()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 4. **Joins**\n",
        "\n",
        "```python\n",
        "df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")\n",
        "df1.join(df2, [\"id\"], \"left_outer\")  # also: right_outer, full_outer, left_semi, left_anti\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 5. **Window Functions**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "windowSpec = Window.partitionBy(\"col1\").orderBy(\"col2\")\n",
        "df.withColumn(\"rank\", F.rank().over(windowSpec))\n",
        "df.withColumn(\"row_num\", F.row_number().over(windowSpec))\n",
        "df.withColumn(\"lag_value\", F.lag(\"value\", 1).over(windowSpec))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 6. **Handling Nulls**\n",
        "\n",
        "```python\n",
        "df.dropna()                            # drop rows with nulls\n",
        "df.fillna({\"col1\": 0, \"col2\": \"N/A\"})  # fill nulls\n",
        "df.na.replace(\"old\", \"new\")            # replace nulls or values\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 7. **UDFs (User Defined Functions)**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def custom_func(x):\n",
        "    return x * 2\n",
        "\n",
        "double_udf = udf(custom_func, IntegerType())\n",
        "df.withColumn(\"doubled\", double_udf(df[\"col\"]))\n",
        "```\n",
        "\n",
        "> ⚠️ Use built-in functions instead of UDFs when possible—they're faster and optimized.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 8. **Spark SQL**\n",
        "\n",
        "```python\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "spark.sql(\"SELECT col1, COUNT(*) FROM my_table GROUP BY col1\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 9. **Repartitioning and Caching**\n",
        "\n",
        "```python\n",
        "df.repartition(10)      # shuffle-based\n",
        "df.coalesce(2)          # reduce partitions (no shuffle)\n",
        "df.cache()              # in-memory cache\n",
        "df.persist()            # cache with storage level\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 10. **Writing to Hive / External Tables**\n",
        "\n",
        "```python\n",
        "df.write.saveAsTable(\"db.table_name\")      # Hive table\n",
        "df.write.option(\"path\", \"hdfs_path\").saveAsTable(\"external_table\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 11. **Schema and Metadata**\n",
        "\n",
        "```python\n",
        "df.printSchema()\n",
        "df.schema\n",
        "df.dtypes\n",
        "df.columns\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 12. **Data Types and Casting**\n",
        "\n",
        "```python\n",
        "df.withColumn(\"col\", df[\"col\"].cast(\"int\"))\n",
        "F.col(\"col\").cast(\"string\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Bonus: **Performance & Debugging**\n",
        "\n",
        "```python\n",
        "df.explain(True)           # physical plan\n",
        "df.rdd.getNumPartitions()  # partition count\n",
        "df.show(5, truncate=False) # display rows\n",
        "```\n",
        "\n",
        "Great question! Beyond the most **commonly used Spark functions** listed earlier, Spark provides a **rich set of additional functions** (especially under `pyspark.sql.functions`) that are helpful in **data cleansing, ETL, feature engineering, and analytics**.\n",
        "\n",
        "Here’s a categorized list of **additional but important Spark functions** that are **less commonly covered but useful in real-world data engineering**:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 String Functions\n",
        "\n",
        "```python\n",
        "F.concat(F.col(\"col1\"), F.col(\"col2\"))       # Concatenate columns\n",
        "F.substring(\"col\", 1, 3)                     # Substring\n",
        "F.upper(\"col\"), F.lower(\"col\")              # Change case\n",
        "F.trim(\"col\"), F.ltrim(\"col\"), F.rtrim(\"col\")\n",
        "F.length(\"col\")                              # String length\n",
        "F.instr(\"col\", \"substr\")                     # Index of substring\n",
        "F.replace(\"col\", \"old\", \"new\")               # Replace substring\n",
        "F.regexp_replace(\"col\", pattern, \"new\")      # Regex replace\n",
        "F.split(\"col\", \",\")                          # Split into array\n",
        "F.concat_ws(\"-\", \"col1\", \"col2\")             # Concatenate with separator\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 Date and Time Functions\n",
        "\n",
        "```python\n",
        "F.current_date(), F.current_timestamp()\n",
        "F.date_format(\"timestamp_col\", \"yyyy-MM-dd\")\n",
        "F.year(\"col\"), F.month(\"col\"), F.dayofmonth(\"col\")\n",
        "F.hour(\"col\"), F.minute(\"col\"), F.second(\"col\")\n",
        "F.date_add(\"date\", 7), F.date_sub(\"date\", 7)\n",
        "F.datediff(\"end_date\", \"start_date\")\n",
        "F.to_date(\"timestamp_col\")\n",
        "F.to_timestamp(\"string_col\", \"yyyy-MM-dd HH:mm:ss\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 Array & Map Functions\n",
        "\n",
        "```python\n",
        "F.array(\"col1\", \"col2\")                      # Create array\n",
        "F.size(\"array_col\")                          # Length of array\n",
        "F.array_contains(\"array_col\", \"value\")\n",
        "F.explode(\"array_col\")                       # Explode array into rows\n",
        "F.map_keys(\"map_col\"), F.map_values(\"map_col\")\n",
        "F.element_at(\"array_col\", 1)                 # Get element by index\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 Conditional Logic\n",
        "\n",
        "```python\n",
        "F.when(F.col(\"age\") > 18, \"adult\").otherwise(\"minor\")\n",
        "F.expr(\"CASE WHEN col > 10 THEN 'high' ELSE 'low' END\")\n",
        "F.isnull(\"col\"), F.isnan(\"col\")              # Check for null or NaN\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 JSON Functions\n",
        "\n",
        "```python\n",
        "F.from_json(\"json_col\", schema)              # Parse JSON\n",
        "F.to_json(\"struct_col\")                      # Convert struct to JSON\n",
        "F.get_json_object(\"json_col\", \"$.field\")     # Extract value from JSON\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 Hashing & Encryption\n",
        "\n",
        "```python\n",
        "F.sha2(\"col\", 256)\n",
        "F.md5(\"col\")\n",
        "F.crc32(\"col\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 Data Skew & Sampling\n",
        "\n",
        "```python\n",
        "df.sample(withReplacement=False, fraction=0.1)\n",
        "df.randomSplit([0.7, 0.3])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 Miscellaneous Useful Functions\n",
        "\n",
        "```python\n",
        "F.monotonically_increasing_id()              # Unique ID generator\n",
        "F.input_file_name()                          # Add source file name as column\n",
        "F.broadcast(df)                              # Broadcast join optimization\n",
        "F.struct(\"col1\", \"col2\")                     # Combine columns into struct\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔸 Partition Functions\n",
        "\n",
        "```python\n",
        "df.write.partitionBy(\"col\").parquet(\"path\")\n",
        "```\n",
        "---\n",
        "\n",
        "### ✅ Tip:\n",
        "\n",
        "You can explore all functions with:\n",
        "\n",
        "```python\n",
        "import pyspark.sql.functions as F\n",
        "dir(F)  # Lists all available functions\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "h6p73wOR_5xU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q16. **Null** and **NaN** with examples:\n",
        "\n",
        "---\n",
        "\n",
        "## What is **NaN**?\n",
        "\n",
        "**NaN** stands for **\"Not a Number\"**. It is a special floating-point value defined by the IEEE 754 standard to represent:\n",
        "\n",
        "* Undefined or unrepresentable numeric results\n",
        "* Missing numeric values distinct from `null`\n",
        "* Results of invalid operations like `0/0`, `∞ - ∞`, or `sqrt(-1)` in some contexts\n",
        "\n",
        "---\n",
        "\n",
        "## Characteristics of NaN:\n",
        "\n",
        "* NaN is a **float/double** type value.\n",
        "* NaN is **not equal** to anything, **including itself**. So `NaN == NaN` is `False`.\n",
        "* Used to indicate invalid or missing numeric data without being a null.\n",
        "* Common in scientific computing and data processing.\n",
        "\n",
        "---\n",
        "\n",
        "## Examples of NaN in Python/Spark context:\n",
        "\n",
        "| Expression                    | Result                   | Explanation                       |\n",
        "| ----------------------------- | ------------------------ | --------------------------------- |\n",
        "| `float('nan')`                | `NaN`                    | Explicit NaN value                |\n",
        "| `0.0 / 0.0`                   | `NaN`                    | Division zero by zero             |\n",
        "| `math.sqrt(-1)`               | Error or NaN             | Imaginary number (depends on lib) |\n",
        "| `float('inf') - float('inf')` | `NaN`                    | Infinity minus infinity           |\n",
        "| Missing float value           | Often represented as NaN | Numeric missing value             |\n",
        "\n",
        "---\n",
        "\n",
        "## Example in PySpark:\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import isnan, isnull, col\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [(1.0,), (float('nan'),), (None,), (3.5,)]\n",
        "df = spark.createDataFrame(data, [\"value\"])\n",
        "\n",
        "df.show()\n",
        "# +-----+\n",
        "# |value|\n",
        "# +-----+\n",
        "# |  1.0|\n",
        "# |  NaN|\n",
        "# | null|\n",
        "# |  3.5|\n",
        "# +-----+\n",
        "\n",
        "# Filter rows where value is NaN\n",
        "df.filter(isnan(col(\"value\"))).show()\n",
        "# +-----+\n",
        "# |value|\n",
        "# +-----+\n",
        "# |  NaN|\n",
        "# +-----+\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Summary:\n",
        "\n",
        "| Null                                | NaN                                       |\n",
        "| ----------------------------------- | ----------------------------------------- |\n",
        "| Represents missing data or no value | Represents an invalid or undefined number |\n",
        "| Applies to all data types           | Only for floating-point types             |\n",
        "| Spark: `F.isnull()`                 | Spark: `F.isnan()`                        |\n",
        "| `None` in Python                    | `float('nan')` in Python                  |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "wUkav0r3KOh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q17. Handling JSON\n",
        "\n",
        "## 🔹 1. **Standard JSON File (JSON Lines / Line-delimited)**\n",
        "\n",
        "Each line is a valid JSON object.\n",
        "\n",
        "### 📁 Example (file: `data1.json`)\n",
        "\n",
        "```json\n",
        "{\"name\": \"Alice\", \"age\": 25}\n",
        "{\"name\": \"Bob\", \"age\": 30}\n",
        "```\n",
        "\n",
        "### ✅ Spark Code\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Read JSON\").getOrCreate()\n",
        "\n",
        "df = spark.read.json(\"data1.json\")\n",
        "df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2. **Multi-line JSON File (Single JSON array or nested object)**\n",
        "\n",
        "Used when a full JSON object spans multiple lines.\n",
        "\n",
        "### 📁 Example (file: `data2.json`)\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"name\": \"Alice\", \"age\": 25},\n",
        "  {\"name\": \"Bob\", \"age\": 30}\n",
        "]\n",
        "```\n",
        "\n",
        "### ✅ Spark Code\n",
        "\n",
        "```python\n",
        "df = spark.read.option(\"multiLine\", True).json(\"data2.json\")\n",
        "df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 3. **Nested JSON**\n",
        "\n",
        "Used when fields are nested JSON objects or arrays.\n",
        "\n",
        "### 📁 Example\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"Alice\",\n",
        "  \"address\": {\n",
        "    \"city\": \"NYC\",\n",
        "    \"zip\": \"10001\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### ✅ Spark Code\n",
        "\n",
        "```python\n",
        "df = spark.read.option(\"multiLine\", True).json(\"nested.json\")\n",
        "df.printSchema()\n",
        "\n",
        "# Access nested fields\n",
        "df.select(\"name\", \"address.city\", \"address.zip\").show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 4. **Array of JSON Objects**\n",
        "\n",
        "Sometimes JSON file contains just an array.\n",
        "\n",
        "### 📁 Example\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"id\": 1, \"value\": \"a\"},\n",
        "  {\"id\": 2, \"value\": \"b\"}\n",
        "]\n",
        "```\n",
        "\n",
        "### ✅ Spark Code\n",
        "\n",
        "```python\n",
        "df = spark.read.option(\"multiLine\", True).json(\"array.json\")\n",
        "df.show()\n",
        "```\n",
        "\n",
        "If Spark complains about a mismatch (e.g. expecting an object not an array), you can wrap it into a Dataset:\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df = spark.read.option(\"multiLine\", True).json(\"array.json\")\n",
        "df = df.select(explode(df).alias(\"item\")).select(\"item.*\")\n",
        "df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 5. **Corrupt / Mixed JSON (bad lines)**\n",
        "\n",
        "Use `mode` and `columnNameOfCorruptRecord` options.\n",
        "\n",
        "### ✅ Spark Code\n",
        "\n",
        "```python\n",
        "df = spark.read.option(\"mode\", \"PERMISSIVE\") \\\n",
        "               .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
        "               .json(\"corrupt.json\")\n",
        "\n",
        "df.show(truncate=False)\n",
        "```\n",
        "\n",
        "Other `mode` options:\n",
        "\n",
        "* `\"PERMISSIVE\"` (default): corrupt records are in a new column.\n",
        "* `\"DROPMALFORMED\"`: drops bad rows.\n",
        "* `\"FAILFAST\"`: fails on first bad row.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 6. **Reading from JSON String (not file)**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "json_str = ['{\"name\":\"Alice\", \"age\":25}', '{\"name\":\"Bob\", \"age\":30}']\n",
        "rdd = spark.sparkContext.parallelize(json_str)\n",
        "df = spark.read.schema(schema).json(rdd)\n",
        "df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Bonus: Write DataFrame to JSON\n",
        "\n",
        "```python\n",
        "df.write.mode(\"overwrite\").json(\"output/path\")\n",
        "```\n",
        "\n",
        "Options:\n",
        "\n",
        "* `mode(\"overwrite\")`, `mode(\"append\")`, etc.\n",
        "* `.option(\"compression\", \"gzip\")` if needed\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0syNTICKN2et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q18. JSON File Flattening\n",
        "\n",
        "Flattening a **nested JSON file** in Spark is a common task—especially for data engineers dealing with hierarchical or semi-structured data (like logs, API responses, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Goal of Flattening\n",
        "\n",
        "Convert this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"Alice\",\n",
        "  \"address\": {\n",
        "    \"city\": \"NYC\",\n",
        "    \"zip\": \"10001\"\n",
        "  },\n",
        "  \"contacts\": [\n",
        "    {\"type\": \"email\", \"value\": \"alice@example.com\"},\n",
        "    {\"type\": \"phone\", \"value\": \"1234567890\"}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "To this:\n",
        "\n",
        "| name  | address.city | address.zip | contacts.type | contacts.value |\n",
        "| ----- | ------------ | ----------- | ------------- | -------------- |\n",
        "| Alice | NYC          | 10001       | email         | alice\\@...     |\n",
        "| Alice | NYC          | 10001       | phone         | 1234567890     |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Step-by-Step PySpark Code to Flatten JSON\n",
        "\n",
        "### 1. **Read and Inspect JSON**\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Flatten JSON\").getOrCreate()\n",
        "\n",
        "df = spark.read.option(\"multiline\", True).json(\"nested.json\")\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Flatten Structs (Nested Objects)**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "flattened_df = df.select(\n",
        "    col(\"name\"),\n",
        "    col(\"address.city\").alias(\"address_city\"),\n",
        "    col(\"address.zip\").alias(\"address_zip\")\n",
        ")\n",
        "flattened_df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Flatten Arrays (Explode Contacts)**\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "contacts_df = df.select(\n",
        "    \"name\",\n",
        "    col(\"address.city\").alias(\"address_city\"),\n",
        "    col(\"address.zip\").alias(\"address_zip\"),\n",
        "    explode(\"contacts\").alias(\"contact\")\n",
        ")\n",
        "\n",
        "flattened_df = contacts_df.select(\n",
        "    \"name\", \"address_city\", \"address_zip\",\n",
        "    col(\"contact.type\").alias(\"contact_type\"),\n",
        "    col(\"contact.value\").alias(\"contact_value\")\n",
        ")\n",
        "\n",
        "flattened_df.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 💾 Writing Flattened Data\n",
        "\n",
        "```python\n",
        "flat_df.write.mode(\"overwrite\").json(\"flattened_output/\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xvfJw7_Wu9dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q19. Client Mode and Cluster Mode.\n",
        "\n",
        "\n",
        "> “In **client mode**, the driver runs on the same machine that submits the Spark job, which is useful during development and testing when you need quick feedback or access to logs. In **cluster mode**, the driver is launched on a node inside the cluster, making it more resilient and better suited for production jobs that need to run reliably without relying on the submitter's machine.”\n",
        "\n",
        "\n",
        "### ✅ **1. Client Mode**\n",
        "\n",
        "* **Driver Location**: The **driver runs on the machine** that submits the Spark job (typically your laptop or an edge node).\n",
        "* **Executors**: Run on the cluster (e.g., YARN, Kubernetes, or standalone).\n",
        "* **Communication**: The client machine must stay **active and reachable** because the cluster communicates back with the driver.\n",
        "\n",
        "#### 🟩 Use Case:\n",
        "\n",
        "* Good for **development**, **debugging**, and **interactive analysis**.\n",
        "* Preferred when you're **submitting jobs from a local or edge machine** with stable network access.\n",
        "\n",
        "#### ⚠️ Limitation:\n",
        "\n",
        "* If the client goes down, the whole job fails.\n",
        "* Not suitable for **long-running jobs** or **unreliable client machines**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **2. Cluster Mode**\n",
        "\n",
        "* **Driver Location**: The **driver runs on one of the nodes** in the cluster (not on the client machine).\n",
        "* **Executors**: Run on other nodes in the cluster as usual.\n",
        "* **Client’s Role**: Submits the job and **exits**; it’s not involved in job execution afterward.\n",
        "\n",
        "#### 🟩 Use Case:\n",
        "\n",
        "* Ideal for **production workloads** and **scheduled jobs**.\n",
        "* Used when you want **reliability** and don’t want the job to depend on your local machine.\n",
        "\n",
        "#### ⚠️ Limitation:\n",
        "\n",
        "* Harder to debug compared to client mode, especially in real-time.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Key Difference Summary:\n",
        "\n",
        "| Feature            | Client Mode                       | Cluster Mode                         |\n",
        "| ------------------ | --------------------------------- | ------------------------------------ |\n",
        "| Driver runs on     | Client (submitter) machine        | Worker node in the cluster           |\n",
        "| Network dependency | Needs stable connection to client | No dependency on client after submit |\n",
        "| Debugging          | Easier (logs on client)           | Harder (logs on cluster)             |\n",
        "| Use case           | Interactive / development         | Production / scheduled workloads     |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qWI8ANL9yMRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q20."
      ],
      "metadata": {
        "id": "HglrbVl502hO"
      }
    }
  ]
}
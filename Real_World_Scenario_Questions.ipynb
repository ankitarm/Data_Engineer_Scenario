{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXq3F10ZYgrmN8yBiCjeay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitarm/Data_Engineer_Scenario/blob/main/Real_World_Scenario_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 - You notice your PySpark job is writing corrupted Parquet files. What do you do?\n",
        " **Step 1: Identify the Issue**\n",
        "\n",
        "* **Check error logs**: Look for `ParquetCorruptException`, `EOFException`.\n",
        "* **Verify output**: Try reading the Parquet files using `spark.read.parquet(path)` or a tool like `parquet-tools` or `AWS Athena`.\n",
        "* **Check sample files**: Validate file sizes‚Äîzero-byte or unusually small files may signal corruption.\n",
        "\n",
        "---\n",
        "\n",
        "  **Step 2: Investigate Common Causes**\n",
        "\n",
        "1. **Schema evolution issues**\n",
        "\n",
        "   * Mismatched schemas between different writes can cause corruption.\n",
        "   * ‚úîÔ∏è Fix: Enable schema merging only if needed:\n",
        "\n",
        "     ```python\n",
        "     spark.read.option(\"mergeSchema\", \"true\")\n",
        "     ```\n",
        "\n",
        "2. **Partial writes or task failures**\n",
        "\n",
        "   * If tasks fail midway, incomplete files may be written.\n",
        "   * ‚úîÔ∏è Fix: Enable job/task retry and speculative execution cautiously:\n",
        "\n",
        "     ```python\n",
        "     spark.conf.set(\"spark.speculation\", \"false\")  # disables speculative execution\n",
        "     ```\n",
        "\n",
        "3. **Concurrent writes**\n",
        "\n",
        "   * Multiple jobs writing to the same location without coordination can corrupt data.\n",
        "   * ‚úîÔ∏è Fix: Ensure only one job writes to a target directory at a time.\n",
        "\n",
        "4. **Incompatible or custom UDFs**\n",
        "\n",
        "   * UDFs that return nulls or un-serializable data can corrupt rows.\n",
        "   * ‚úîÔ∏è Fix: Validate data types and sanitize values before writing.\n",
        "\n",
        "5. **File system issues (e.g., S3 eventual consistency)**\n",
        "\n",
        "   * On S3 or HDFS, delayed consistency can result in missing metadata.\n",
        "   * ‚úîÔ∏è Fix: Use **`spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2`**\n",
        "     to reduce write inconsistency.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **Step 3: Isolate the Bad Files**\n",
        "\n",
        "* Use `parquet-tools`:\n",
        "\n",
        "  ```bash\n",
        "  parquet-tools schema part-*.parquet\n",
        "  ```\n",
        "* Delete or quarantine corrupted files and rerun the failed part of the job if possible.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Step 4: Apply Best Practices**\n",
        "\n",
        "* Write to a temp directory and move to the final location only if the job succeeds.\n",
        "* Use `.coalesce(n)` to reduce the number of small files.\n",
        "* Enable write validation:\n",
        "\n",
        "  ```python\n",
        "  spark.conf.set(\"parquet.enable.summary-metadata\", \"true\")\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "vJ_660zEe_UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Q2 - Your Spark job fails when reading JSON files with inconsistent schema. What‚Äôs your fix?\n",
        "\n",
        " The problem is that different JSON files might have different structures (e.g., missing fields, different data types for the same field, or new fields).\n",
        " Spark, by default, when reading JSON, infers the schema from the first file it reads (or from a sample if set). If subsequent files have a different structure, the job may fail.\n",
        " There are several ways to handle this:\n",
        " 1. **Schema Merging**: Spark has an option to merge schemas from multiple JSON files. This is especially useful when the JSON files have different fields (but note: it might not help with type conflicts).\n",
        " 2. **Define a Fixed Schema**: If we know the expected schema, we can define it explicitly and then use that schema to read the JSON files. This way, any field not present in the schema will be ignored, and any missing field will be set to null. However, if there is a type conflict (e.g., a field is integer in one file and string in another), then we might still get an error.\n",
        " 3. **Use `permissive` mode with a column for corrupt records**: We can set the mode to `PERMISSIVE` (which is the default) and also use the `columnNameOfCorruptRecord` option. This will put the malformed records (due to schema inconsistency) into a separate column. Then we can handle them separately.\n",
        " 4. **Use `mode` option for parsing**: We can set the mode when reading JSON to `DROPMALFORMED` or `FAILFAST`. However, `FAILFAST` would fail quickly on any malformed record (which we are already experiencing) and `DROPMALFORMED` would drop the records that don't match the inferred or provided schema. But note: `DROPMALFORMED` might drop entire records if one field is inconsistent.\n",
        " 5. **Preprocess the JSON files**: We might preprocess the JSON files to make them consistent (e.g., using a script or a preliminary Spark job to standardize the schema).\n",
        " 6. **Use a more flexible format**: If the inconsistency is too high, we might consider using a more flexible format like Parquet (with schema merging) or ORC, but that requires converting the data."
      ],
      "metadata": {
        "id": "BePf3FCHizbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Q3 - You have multiple joins in your ETL. How do you debug performance?\n",
        "\n",
        "\n",
        "1. **Inspect Spark UI** for slow stages/shuffles.\n",
        "  - **Shuffle Metrics**: High shuffle read/write (`spark.shuffle.spill`, `shuffle bytes`) indicates costly joins.\n",
        "\n",
        "- Look for warnings like:\n",
        "  ```\n",
        "  WARN JoinSelection: Detected cartesian product join (expensive!)\n",
        "  ```\n",
        "2. **Broadcast small tables** where possible.\n",
        "3. **Handle skew** (salting, split processing).\n",
        "- **Skewed keys** cause uneven task workloads (some tasks take much longer).\n",
        "- Check skew in join keys:\n",
        "  ```scala\n",
        "  df.groupBy(\"join_key\").count().orderBy(desc(\"count\")).show()\n",
        "  ```\n",
        "- **Mitigate Skew**:\n",
        "  ```scala\n",
        "  // Option 1: Salting (add random prefixes to skewed keys)\n",
        "  val saltedDf = df.withColumn(\"salted_key\", concat($\"key\", lit(\"_\"), (rand() * 10).cast(\"int\")))\n",
        "  \n",
        "  // Option 2: Split skewed keys into separate processing\n",
        "  val skewedKeys = Seq(\"key1\", \"key2\")  // Manually identified\n",
        "  val skewedData = df1.filter($\"key\".isin(skewedKeys: _*))\n",
        "  val nonSkewedData = df1.filter(!$\"key\".isin(skewedKeys: _*))\n",
        "  ```\n",
        "\n",
        "4. **Reorder joins** and enable CBO.\n",
        "- Spark evaluates joins left-to-right by default. **Reorder joins** to:\n",
        "  - Filter early (reduce data volume).\n",
        "  - Place smaller tables first to enable broadcast joins.\n",
        "\n",
        "5. **Tune shuffle partitions** and memory.\n",
        "\n",
        "**6. Debug with EXPLAIN**\n",
        "- Use `EXPLAIN` to inspect the physical plan:\n",
        "  ```scala\n",
        "  df.explain(\"extended\")  // Show parsed/logical/physical plans\n",
        "  ```\n",
        "\n",
        "\n",
        "6. **Cache** reused DataFrames.\n",
        "\n",
        "- **Cache** reused DataFrames to avoid recomputation:\n",
        "  ```scala\n",
        "  df.cache().count()  // Materialize cache\n",
        "  ```\n",
        "- **Monitor Cache Usage** in Spark UI‚Äôs \"Storage\" tab.\n",
        "\n",
        "7. **Common Anti-Patterns to Fix**\n",
        " ```\n",
        "| Issue | Solution |\n",
        "|-------|----------|\n",
        "| Cartesian joins | Add explicit join conditions or disable (`spark.sql.crossJoin.enabled=false`) |\n",
        "| Joining on mismatched types | Cast keys to the same type (e.g., `df.withColumn(\"key\", $\"key\".cast(\"int\"))`) |\n",
        "| Unpartitioned data | Repartition before join: `df.repartition(100, $\"key\")` |\n",
        "```\n"
      ],
      "metadata": {
        "id": "6kEZrllAyb5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4 - Data duplication is found after a union. What‚Äôs wrong?\n",
        "\n",
        "\n",
        " **Root Causes**\n",
        "1. **`union` vs. `unionAll` Misunderstanding**:\n",
        "   - In Spark, `union` (and `unionAll`, which is an alias) **does not deduplicate data**. It simply stacks datasets on top of each other, including duplicates.\n",
        "   - If you expected deduplication (like SQL‚Äôs `UNION DISTINCT`), you used the wrong operation.\n",
        "\n",
        "2. **Overlapping Data in Source Datasets**:\n",
        "   - The input DataFrames/RDDs being unioned contain overlapping records (e.g., identical rows across files or partitions).\n",
        "\n",
        "3. **Inconsistent Data Generation**:\n",
        "   - Upstream ETL logic (e.g., joins, aggregations) is producing duplicate rows before the `union`.\n",
        "\n",
        "---\n",
        "\n",
        " **How to Debug**\n",
        "1. **Check for Duplicates in Inputs**:\n",
        "   ```python\n",
        "   # Count distinct rows in each DataFrame before union\n",
        "   df1.distinct().count() == df1.count()  # False means duplicates exist\n",
        "   df2.distinct().count() == df2.count()\n",
        "   ```\n",
        "\n",
        "2. **Inspect the Union Logic**:\n",
        "   ```python\n",
        "   # Show duplicates introduced by union\n",
        "   df_union = df1.union(df2)\n",
        "   df_union.groupBy(df_union.columns).count().filter(\"count > 1\").show()\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        " **Solutions**\n",
        " **1. Use `union` + `distinct` (If Deduplication Is Needed)**\n",
        "   ```python\n",
        "   df_union_distinct = df1.union(df2).distinct()  # Removes duplicates globally\n",
        "   ```\n",
        "   - **Tradeoff**: Expensive (requires full data shuffle).\n",
        "\n",
        " **2. Use `unionByName` (If Schemas Differ Slightly)**\n",
        "   ```python\n",
        "   df_union = df1.unionByName(df2)  # Matches columns by name, not position\n",
        "   ```\n",
        "   - Prevents mismatched columns from creating \"hidden\" duplicates.\n",
        "\n",
        " **3. Deduplicate Before Union (Optimal for Performance)**\n",
        "   ```python\n",
        "   df_union = df1.distinct().union(df2.distinct())  # Dedupe inputs first\n",
        "   ```\n",
        "   - **Best for**: Large datasets where post-union `distinct` is costly.\n",
        "\n",
        " **4. Use `except` to Find Overlaps (Debugging)**\n",
        "   ```python\n",
        "   df1.intersect(df2).show()  # Show overlapping rows\n",
        "   ```\n",
        "\n",
        " **5. Fix Upstream Logic**\n",
        "   - If duplicates come from source data (e.g., double-reading files), adjust the ETL:\n",
        "     ```python\n",
        "     # Example: Use .dropDuplicates() after reading\n",
        "     df1 = spark.read.parquet(\"path\").dropDuplicates()\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Notes**\n",
        "- **`union` ‚â† `UNION DISTINCT`**: Spark‚Äôs `union` behaves like SQL‚Äôs `UNION ALL`.\n",
        "- **Performance**: Post-union `distinct` is expensive; prefer deduping inputs first.\n",
        "- **Schema Safety**: Use `unionByName` if columns are in different orders.\n",
        "\n",
        "---\n",
        "\n",
        " **Example Workflow**\n",
        "```python\n",
        "# Read and deduplicate inputs\n",
        "df1 = spark.read.parquet(\"path1\").dropDuplicates([\"id\", \"timestamp\"])\n",
        "df2 = spark.read.parquet(\"path2\").dropDuplicates([\"id\", \"timestamp\"])\n",
        "\n",
        "# Union with distinct (if needed)\n",
        "df_final = df1.unionByName(df2).distinct()\n",
        "```\n",
        "\n",
        "By addressing the root cause (usually upstream logic or misunderstanding of `union`), you can eliminate duplicates efficiently.\n"
      ],
      "metadata": {
        "id": "Z4vKCCPzkncc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 - You‚Äôre required to load only new data each day. How do you handle incremental loads?\n",
        "\n",
        " **1. Track Incremental Data Sources**\n",
        "Identify how to detect new data:\n",
        "- **Timestamp-based**: Use a column like `last_updated_at` or `event_time`.\n",
        "- **ID-based**: Incrementing IDs (e.g., `auto_increment` keys).\n",
        "- **File-based**: New files arrive in a directory (e.g., `s3://bucket/day=2024-01-01/`).\n",
        "- **CDC (Change Data Capture)**: Log-based tools (Debezium, AWS DMS) or database triggers.\n",
        "\n",
        "---\n",
        "\n",
        " **2. Store the \"High-Water Mark\"**\n",
        "Persist the last processed record to avoid reprocessing:\n",
        "- **Options**:\n",
        "  - Database table (e.g., `metadata.load_history`).\n",
        "  - File (e.g., `_last_processed.json` in cloud storage).\n",
        "  - Spark‚Äôs `checkpoint` (for streaming jobs).\n",
        "\n",
        "**Example (Delta Lake)**:\n",
        "```python\n",
        "# Read last processed timestamp\n",
        "last_run = spark.read.json(\"dbfs:/mnt/last_processed.json\").first()[\"last_timestamp\"]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Query Only New Data**\n",
        "Filter data at read time using the high-water mark:\n",
        " **For Timestamp-Based Increments**\n",
        "```python\n",
        "new_data = spark.read.table(\"source_table\") \\\n",
        "  .filter(f\"event_time > '{last_run}'\")  # Or use >= for inclusivity\n",
        "```\n",
        "\n",
        " **For File-Based Increments (e.g., Hive-Style Partitioning)**\n",
        "```python\n",
        "new_data = spark.read.parquet(\"s3://bucket/\") \\\n",
        "  .filter(\"day > '2024-01-01'\")  # Dynamic partition filtering\n",
        "```\n",
        "\n",
        " **For Database CDC**\n",
        "```python\n",
        "# Using Debezium output (e.g., Kafka topic)\n",
        "cdc_data = spark.read.format(\"kafka\") \\\n",
        "  .option(\"startingOffsets\", last_kafka_offset) \\\n",
        "  .load()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Merge or Append New Data**\n",
        " **Option A: Append (Immutable Data)**\n",
        "```python\n",
        "new_data.write.mode(\"append\").saveAsTable(\"target_table\")\n",
        "```\n",
        "\n",
        " **Option B: Merge (Updates + Inserts)**\n",
        "Use Delta Lake/Iceberg for ACID operations:\n",
        "```python\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_table = DeltaTable.forPath(spark, \"/path/to/target\")\n",
        "delta_table.alias(\"target\").merge(\n",
        "  new_data.alias(\"source\"),\n",
        "  \"target.id = source.id\") \\\n",
        "  .whenMatchedUpdateAll() \\\n",
        "  .whenNotMatchedInsertAll() \\\n",
        "  .execute()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Update the High-Water Mark**\n",
        "After successful processing:\n",
        "```python\n",
        "# Update timestamp (e.g., max(new_data.event_time))\n",
        "new_max = new_data.selectExpr(\"max(event_time)\").first()[0]\n",
        "\n",
        "# Persist for next run\n",
        "spark.createDataFrame([(new_max,)], [\"last_timestamp\"]) \\\n",
        "  .write.mode(\"overwrite\").json(\"dbfs:/mnt/last_processed.json\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Handle Failures Idempotently**\n",
        "- **Idempotent writes**: Use transactional formats (Delta Lake, Iceberg) to avoid partial loads.\n",
        "- **Checkpointing**: In streaming jobs, use:\n",
        "  ```python\n",
        "  spark.readStream.option(\"checkpointLocation\", \"/checkpoint/path\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **7. Optimize Performance**\n",
        "- **Partitioning**: Ensure target tables are partitioned by date/ID for fast filtering.\n",
        "- **Z-Ordering** (Delta Lake): Cluster by key columns to speed up merges:\n",
        "  ```python\n",
        "  delta_table.optimize().executeZOrderBy(\"id\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **Example End-to-End Pipeline**\n",
        "```python\n",
        "# 1. Fetch last processed timestamp\n",
        "last_run = spark.read.json(\"last_processed.json\").first()[\"timestamp\"]\n",
        "\n",
        "# 2. Load new data\n",
        "new_data = spark.read.table(\"source\").filter(f\"timestamp > '{last_run}'\")\n",
        "\n",
        "# 3. Merge into target\n",
        "delta_table = DeltaTable.forPath(spark, \"/data/target\")\n",
        "delta_table.alias(\"t\").merge(\n",
        "  new_data.alias(\"s\"),\n",
        "  \"t.id = s.id\") \\\n",
        "  .whenMatchedUpdateAll() \\\n",
        "  .whenNotMatchedInsertAll() \\\n",
        "  .execute()\n",
        "\n",
        "# 4. Update high-water mark\n",
        "new_max = new_data.selectExpr(\"max(timestamp)\").first()[0]\n",
        "spark.createDataFrame([(new_max,)], [\"timestamp\"]) \\\n",
        "  .write.mode(\"overwrite\").json(\"last_processed.json\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Considerations**\n",
        "| Approach | Best For | Tools |\n",
        "|----------|----------|-------|\n",
        "| **Timestamp/ID** | Append-only data | Delta Lake, plain Parquet |\n",
        "| **File-Based** | Object storage (S3, HDFS) | Hive partitions, AutoLoader |\n",
        "| **CDC** | Database updates | Debezium, Kafka, Delta Lake CDC |\n"
      ],
      "metadata": {
        "id": "yRzqg-trmF5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6 - Your PySpark notebook is stuck at an action like `collect()`. What‚Äôs your approach?\n",
        "\n",
        "\n",
        " **Summary Checklist**\n",
        "1. **Check Spark UI** for stuck stages/skew.\n",
        "2. **Avoid `collect()`**‚Äîuse `take()`, `show()`, or writes.\n",
        "3. **Repartition/salt data** if skewed.\n",
        "4. **Increase memory** (driver/executors) if OOM occurs.\n",
        "5. **Optimize queries** (filter early, checkpoint).\n",
        "6. **Tune Spark configs** (shuffle partitions, timeouts).\n",
        "\n",
        " **1. Diagnose the Problem**\n",
        " **Check the Spark UI**\n",
        "- Access the Spark UI (`http://<driver-node>:4040`) while the job is stuck:\n",
        "  - **Stages Tab**: Identify which stage/task is hanging (look for incomplete/failed tasks).\n",
        "  - **Executors Tab**: Check if executors are alive, memory usage, and GC (garbage collection) time.\n",
        "  - **Storage Tab**: Verify if excessive caching is causing memory pressure.\n",
        "\n",
        " **Look for Warnings/Errors**\n",
        "- **Driver/Executor logs** (accessible via Spark UI or cluster logs):\n",
        "  - OutOfMemory (OOM) errors.\n",
        "  - Long garbage collection pauses (`GC overhead limit exceeded`).\n",
        "  - Network timeouts (`TimeoutException`).\n",
        "\n",
        "---\n",
        "\n",
        " **2. Immediate Mitigations**\n",
        " **Cancel and Reprocess Safely**\n",
        "- **Keyboard interrupt** (if in notebook) or stop the Spark job via UI.\n",
        "- **Avoid `collect()`** for large datasets:\n",
        "  ```python\n",
        "  # Replace:\n",
        "  df.collect()  # Brings all data to driver (risky for big data)\n",
        "  \n",
        "  # With safer actions:\n",
        "  df.take(100)  # Fetch a sample\n",
        "  df.write.parquet(\"output.parquet\")  # Write to disk instead\n",
        "  ```\n",
        "\n",
        " **Increase Resources (If Possible)**\n",
        "- **Driver Memory**: Set higher memory for the driver (e.g., `--driver-memory 8G`).\n",
        "- **Executor Memory/Instances**:\n",
        "  ```python\n",
        "  spark.conf.set(\"spark.executor.memory\", \"4G\")\n",
        "  spark.conf.set(\"spark.executor.instances\", \"10\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Debug Common Causes**\n",
        " **Cause 1: Data Skew**\n",
        "- **Symptoms**: A few tasks run much longer than others.\n",
        "- **Fix**:\n",
        "  - **Salting**: Redistribute skewed keys:\n",
        "    ```python\n",
        "    from pyspark.sql.functions import rand\n",
        "    df = df.withColumn(\"salted_key\", concat(\"key\", lit(\"_\"), (rand() * 10).cast(\"int\")))\n",
        "    ```\n",
        "  - **Repartition**: Balance partitions:\n",
        "    ```python\n",
        "    df = df.repartition(200, \"key\")  # Increase partition count\n",
        "    ```\n",
        "\n",
        " **Cause 2: Memory Issues**\n",
        "- **Symptoms**: `java.lang.OutOfMemoryError` or high spill-to-disk.\n",
        "- **Fix**:\n",
        "  - **Reduce partition size**:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # Default: 200\n",
        "    ```\n",
        "  - **Cache wisely**:\n",
        "    ```python\n",
        "    df.cache().count()  # Materialize cache\n",
        "    df.unpersist()  # Release if unused\n",
        "    ```\n",
        "\n",
        " **Cause 3: Expensive Transformations Before `collect()`**\n",
        "- **Symptoms**: Slow stages before the action.\n",
        "- **Fix**:\n",
        "  - **Optimize queries**:\n",
        "    ```python\n",
        "    # Replace:\n",
        "    df.filter(\"col1 > 100\").collect()\n",
        "    \n",
        "    # With pushed-down filtering:\n",
        "    df.select(\"col1\").filter(\"col1 > 100\").collect()\n",
        "    ```\n",
        "  - **Checkpoint intermediate results**:\n",
        "    ```python\n",
        "    df.checkpoint()  # Breaks lineage for complex DAGs\n",
        "    ```\n",
        "\n",
        " **Cause 4: Network/Dependency Issues**\n",
        "- **Symptoms**: Tasks hanging at \"shuffle\" or external calls.\n",
        "- **Fix**:\n",
        "  - **Increase timeouts**:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.network.timeout\", \"600s\")\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Alternative Actions to `collect()`**\n",
        "- **For inspection**:\n",
        "  ```python\n",
        "  df.show(10)        # Prints rows to console\n",
        "  df.take(100)       # Returns a list of rows (limited size)\n",
        "  df.limit(100).collect()  # Safer than full collect\n",
        "  ```\n",
        "- **For writing**:\n",
        "  ```python\n",
        "  df.write.csv(\"output.csv\")  # Avoids driver memory issues\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Long-Term Prevention**\n",
        "- **Monitor**: Use Spark UI regularly to catch skew/memory issues early.\n",
        "- **Tune configurations**:\n",
        "  ```python\n",
        "  spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # Auto-optimize shuffle\n",
        "  spark.conf.set(\"spark.executor.memoryOverhead\", \"2G\")  # For off-heap memory\n",
        "  ```\n",
        "- **Use structured streaming** for incremental processing instead of batch `collect()`.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cMmDIbY_s4Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7 - You're asked to validate the data pipeline output against the source. How?\n",
        "\n",
        " **1. Define Validation Criteria**\n",
        "Start by identifying key metrics to compare between source and target:\n",
        "- **Row Count**: Record counts should match (or meet expected differences, like deduplication).\n",
        "- **Schema**: Column names, data types, and nullability.\n",
        "- **Data Integrity**: Key fields (e.g., IDs, timestamps) should have no unexpected changes.\n",
        "- **Business Logic**: Aggregations, transformations, and derived fields must adhere to rules.\n",
        "- **Freshness**: Data should be up-to-date (e.g., latest timestamps match SLAs).\n",
        "\n",
        "---\n",
        "\n",
        " **2. Automated Validation Checks**\n",
        "Implement programmatic checks using PySpark/SQL:\n",
        "\n",
        " **A. Row Count Validation**\n",
        "```python\n",
        "source_count = spark.table(\"source_table\").count()\n",
        "target_count = spark.table(\"target_table\").count()\n",
        "\n",
        "assert source_count == target_count, \\\n",
        "    f\"Count mismatch: Source={source_count}, Target={target_count}\"\n",
        "```\n",
        "\n",
        " **B. Schema Validation**\n",
        "```python\n",
        "source_schema = spark.table(\"source_table\").schema\n",
        "target_schema = spark.table(\"target_table\").schema\n",
        "\n",
        " source_schema == target_schema, \\\n",
        "    \"Schema mismatch!\\nSource:\\n{source_schema}\\nTarget:\\n{target_schema}\"\n",
        "```\n",
        "\n",
        " **C. Data Diff (Sample or Full)**\n",
        "Compare a sample of records:\n",
        "```python\n",
        "from pyspark.sql.functions import md5, concat_ws\n",
        "\n",
        "# Compare checksums of key columns\n",
        "source_hashes = spark.table(\"source_table\") \\\n",
        "    .select(md5(concat_ws(\"|\", \"id\", \"timestamp\")).alias(\"hash\"))\n",
        "target_hashes = spark.table(\"target_table\") \\\n",
        "    .select(md5(concat_ws(\"|\", \"id\", \"timestamp\")).alias(\"hash\"))\n",
        "\n",
        "# Find mismatches\n",
        "mismatches = source_hashes.join(target_hashes, \"hash\", \"anti\")\n",
        "assert mismatches.count() == 0, f\"{mismatches.count()} mismatched records found\"\n",
        "```\n",
        "\n",
        " **D. Statistical Validation**\n",
        "Check distributions/aggregates:\n",
        "```python\n",
        "source_stats = spark.table(\"source_table\").selectExpr(\n",
        "    \"avg(revenue) as avg_revenue\",\n",
        "    \"count(distinct user_id) as distinct_users\"\n",
        ").first()\n",
        "\n",
        "target_stats = spark.table(\"target_table\").selectExpr(\n",
        "    \"avg(revenue) as avg_revenue\",\n",
        "    \"count(distinct user_id) as distinct_users\"\n",
        ").first()\n",
        "\n",
        "assert round(source_stats.avg_revenue, 2) == round(target_stats.avg_revenue, 2), \\\n",
        "    \"Revenue averages differ\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Handle Expected Deltas**\n",
        "If the pipeline intentionally modifies data (e.g., filtering, deduplication), adjust validations:\n",
        "```python\n",
        "# Example: Expect 5% fewer records due to deduplication\n",
        "expected_count = source_count * 0.95\n",
        "assert abs(target_count - expected_count) < 100, \\\n",
        "    \"Count outside expected tolerance\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Logging and Alerting**\n",
        "- **Log discrepancies** for triage:\n",
        "  ```python\n",
        "  if mismatches.count() > 0:\n",
        "      mismatches.write.mode(\"overwrite\").parquet(\"path/to/mismatches\")\n",
        "  ```\n",
        "- **Set up alerts** (e.g., via Airflow, Databricks Jobs, or custom monitors) for failures.\n",
        "\n",
        "---\n",
        "\n",
        " **5. Source-to-Target Tools (For Large Pipelines)**\n",
        "For complex pipelines, consider specialized tools:\n",
        "- **Great Expectations**: Framework for data validation.\n",
        "  ```python\n",
        "  import great_expectations as ge\n",
        "  df = ge.dataset.SparkDFDataset(spark.table(\"target_table\"))\n",
        "  df.expect_column_values_to_not_be_null(\"user_id\")\n",
        "  ```\n",
        "- **Delta Lake/DBT Tests**: Built-in assertions for schema/data quality.\n",
        "  ```sql\n",
        "  -- In DBT\n",
        "  tests:\n",
        "    - not_null:\n",
        "        column: user_id\n",
        "    - relationships:\n",
        "        to: ref('source_table')\n",
        "        field: id\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Edge Cases to Validate**\n",
        "- **Null Handling**: Ensure `NULL` values are preserved or transformed correctly.\n",
        "- **Data Type Conversions**: Verify no loss of precision (e.g., `timestamp` ‚Üí `string`).\n",
        "- **Partitioning**: Check if partition columns match source predicates.\n",
        "- **Incremental Loads**: Validate only new/updated data was processed.\n",
        "\n",
        "---\n",
        "\n",
        " **7. Example Validation Workflow**\n",
        "```python\n",
        "def validate_pipeline():\n",
        "    source = spark.table(\"source\").filter(\"date = '2023-01-01'\")\n",
        "    target = spark.table(\"target\").filter(\"date = '2023-01-01'\")\n",
        "    \n",
        "    # Row count\n",
        "    assert source.count() == target.count(), \"Row count mismatch\"\n",
        "    \n",
        "    # Schema\n",
        "    assert set(source.columns) == set(target.columns), \"Column mismatch\"\n",
        "    \n",
        "    # Key field consistency\n",
        "    source_ids = source.select(\"id\").distinct()\n",
        "    target_ids = target.select(\"id\").distinct()\n",
        "    assert source_ids.subtract(target_ids).count() == 0, \"ID mismatch\"\n",
        "    \n",
        "    print(\"Validation passed!\")\n",
        "\n",
        "validate_pipeline()\n",
        "```\n",
        "\n",
        "---\n",
        "```\n",
        " **Key Takeaways**\n",
        "| **Check**               | **Method**                                                                 |\n",
        "|-------------------------|----------------------------------------------------------------------------|\n",
        "| **Row Count**           | Compare `.count()` between source/target.                                  |\n",
        "| **Schema**              | Validate column names, types, and nullability.                             |\n",
        "| **Data Accuracy**       | Use checksums (e.g., `md5`) or join anti-checks.                           |\n",
        "| **Business Logic**      | Test aggregations (e.g., `sum(revenue)`).                                  |\n",
        "| **Automation**          | Integrate with CI/CD (e.g., GitHub Actions) or orchestration (Airflow).    |\n",
        "```"
      ],
      "metadata": {
        "id": "PyRC-5jOwbrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8 - You're asked to convert a nested JSON into a flat table. What steps do you take?\n",
        "\n",
        "\n",
        "Converting nested JSON into a flat table in PySpark involves **flattening nested structures** (arrays, structs) into columns. Here‚Äôs a step-by-step guide:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Read the Nested JSON**\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df = spark.read.json(\"path/to/nested_data.json\")  # Or use `option(\"multiline\", \"true\")` for multi-line JSON\n",
        "df.printSchema()\n",
        "```\n",
        "**Sample Nested Schema**:\n",
        "```json\n",
        "{\n",
        "  \"id\": 1,\n",
        "  \"name\": \"Alice\",\n",
        "  \"address\": {\n",
        "    \"city\": \"NY\",\n",
        "    \"zip\": 10001\n",
        "  },\n",
        "  \"orders\": [\n",
        "    {\"order_id\": 101, \"amount\": 50.0},\n",
        "    {\"order_id\": 102, \"amount\": 30.0}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **2. Flatten Structs (Single Nested Fields)**\n",
        "Use dot notation or `selectExpr` to unnest structs:\n",
        "```python\n",
        "# Method 1: Dot notation\n",
        "flattened_df = df.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    col(\"address.city\").alias(\"city\"),  # Unnest struct\n",
        "    col(\"address.zip\").alias(\"zip\")\n",
        ")\n",
        "\n",
        "# Method 2: selectExpr (SQL-like syntax)\n",
        "flattened_df = df.selectExpr(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    \"address.city as city\",\n",
        "    \"address.zip as zip\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Explode Arrays (List of Objects)**\n",
        "For arrays, use `explode()` or `explode_outer()` (to keep nulls):\n",
        "```python\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# Explode the \"orders\" array\n",
        "exploded_df = df.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    explode(\"orders\").alias(\"order\")  # Each array element becomes a row\n",
        ")\n",
        "\n",
        "# Then flatten the exploded struct\n",
        "final_df = exploded_df.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    col(\"order.order_id\").alias(\"order_id\"),\n",
        "    col(\"order.amount\").alias(\"amount\")\n",
        ")\n",
        "```\n",
        "**Output**:\n",
        "```\n",
        "+---+-----+--------+------+\n",
        "|id |name |order_id|amount|\n",
        "+---+-----+--------+------+\n",
        "|1  |Alice|101     |50.0  |\n",
        "|1  |Alice|102     |30.0  |\n",
        "+---+-----+--------+------+\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Handle Nested Arrays (Advanced)**\n",
        "For multi-level nesting, combine `explode` with `struct` flattening:\n",
        "```python\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# Example: JSON with nested arrays\n",
        "df = spark.read.json(\"path/to/deeply_nested.json\")\n",
        "\n",
        "# Step 1: Explode outer array\n",
        "df_exploded = df.withColumn(\"order_item\", explode(\"orders.items\"))\n",
        "\n",
        "# Step 2: Flatten structs\n",
        "df_flat = df_exploded.select(\n",
        "    \"id\",\n",
        "    col(\"order_item.item_id\").alias(\"item_id\"),\n",
        "    col(\"order_item.price\").alias(\"price\")\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Dynamic Flattening (For Unknown Schemas)**\n",
        "If the schema is unknown or highly dynamic:\n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def flatten_df(nested_df):\n",
        "    flat_cols = []\n",
        "    nested_cols = []\n",
        "    \n",
        "    for field in nested_df.schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            # Recursively flatten structs\n",
        "            for sub_field in field.dataType.fields:\n",
        "                flat_cols.append(col(f\"{field.name}.{sub_field.name}\").alias(f\"{field.name}_{sub_field.name}\"))\n",
        "        else:\n",
        "            flat_cols.append(col(field.name))\n",
        "    \n",
        "    return nested_df.select(flat_cols)\n",
        "\n",
        "flattened_df = flatten_df(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Write the Flat Table**\n",
        "```python\n",
        "flattened_df.write.parquet(\"path/to/flattened_table\")  # Or CSV/Delta\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Functions & Tips**\n",
        "| **Function**           | **Purpose**                                                                 |\n",
        "|------------------------|----------------------------------------------------------------------------|\n",
        "| `explode()`            | Converts array elements into rows (skips nulls/empty arrays).              |\n",
        "| `explode_outer()`      | Includes null/empty arrays in output.                                      |\n",
        "| `selectExpr()`         | SQL-like syntax for complex column operations.                             |\n",
        "| `struct.field`         | Dot notation to access nested struct fields.                               |\n",
        "| `from_json()`          | Parse JSON strings into structs before flattening.                         |\n",
        "\n",
        "**Performance Tips**:\n",
        "- **Filter early**: Use `select()` to prune unused columns before flattening.\n",
        "- **Limit explosions**: Exploding large arrays can cause row multiplication (use `size()` to check).\n",
        "- **Schema inference**: Use `spark.read.json` with sampling for large files:\n",
        "  ```python\n",
        "  spark.read.option(\"samplingRatio\", \"0.1\").json(\"path\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **Example: End-to-End Flattening**\n",
        "```python\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "# Input JSON with nested fields and arrays\n",
        "data = [\n",
        "    {\"id\": 1, \"name\": \"Alice\", \"address\": {\"city\": \"NY\", \"zip\": 10001}, \"orders\": [{\"id\": 101, \"amount\": 50.0}]},\n",
        "    {\"id\": 2, \"name\": \"Bob\", \"address\": {\"city\": \"SF\", \"zip\": 94105}, \"orders\": []}\n",
        "]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Step 1: Flatten structs\n",
        "df_flat = df.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    col(\"address.city\").alias(\"city\"),\n",
        "    col(\"address.zip\").alias(\"zip\"),\n",
        "    \"orders\"\n",
        ")\n",
        "\n",
        "# Step 2: Explode arrays (with explode_outer to keep Bob's empty orders)\n",
        "df_final = df_flat.select(\n",
        "    \"id\", \"name\", \"city\", \"zip\",\n",
        "    explode_outer(\"orders\").alias(\"order\")\n",
        ").select(\n",
        "    \"*\",\n",
        "    col(\"order.id\").alias(\"order_id\"),\n",
        "    col(\"order.amount\").alias(\"amount\")\n",
        ").drop(\"order\")\n",
        "\n",
        "df_final.show()\n",
        "```\n",
        "**Output**:\n",
        "```\n",
        "+---+-----+----+-----+--------+------+\n",
        "| id| name|city|  zip|order_id|amount|\n",
        "+---+-----+----+-----+--------+------+\n",
        "|  1|Alice|  NY|10001|     101|  50.0|\n",
        "|  2|  Bob|  SF|94105|    null|  null|\n",
        "+---+-----+----+-----+--------+------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "LX8bci9V0CSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9 - The job fails with 'Files were not closed properly'. What might be the issue?\n",
        "The error **\"Files were not closed properly\"** in Spark typically indicates issues with file handles or I/O operations during job execution. Here are the common causes and solutions:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Resource Leaks (Most Common)**\n",
        "#### **Cause**:\n",
        "- **Unclosed File Handles**: Spark couldn‚Äôt close output files (e.g., Parquet, ORC) due to:\n",
        "  - Executor crashes or JVM failures.\n",
        "  - Improper shutdown (e.g., `kill -9` or cluster preemption).\n",
        "  - Code not calling `.close()` on `OutputStream`s in UDFs.\n",
        "\n",
        "#### **Fix**:\n",
        "- **Graceful Shutdown**: Ensure jobs exit cleanly (avoid forced termination).\n",
        "- **Use `try-finally`** for resource management:\n",
        "  ```python\n",
        "  try:\n",
        "      df.write.parquet(\"output_path\")\n",
        "  finally:\n",
        "      spark.stop()  # Ensures SparkContext cleanup\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Disk/Network Issues**\n",
        "#### **Cause**:\n",
        "- **Storage Disconnects**: Network timeouts or disk failures during writes (e.g., S3, HDFS).\n",
        "- **Permission Denied**: Filesystem permissions changed mid-job.\n",
        "\n",
        "#### **Fix**:\n",
        "- **Check Storage Health**:\n",
        "  ```bash\n",
        "  hdfs fsck /output_path  # For HDFS\n",
        "  aws s3 ls s3://bucket/output_path/  # For S3\n",
        "  ```\n",
        "- **Retry Logic**: Use `spark.hadoop.fs.s3a.retry.limit` (S3) or similar for HDFS.\n",
        "- **Validate Permissions**:\n",
        "  ```bash\n",
        "  hdfs dfs -ls /output_path  # Verify write permissions\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Spark Configuration Gaps**\n",
        "#### **Cause**:\n",
        "- Missing settings for fault-tolerant writes (especially in cloud storage).\n",
        "\n",
        "#### **Fix**:\n",
        "- **Enable Committer Algorithms** (for S3/S3A):\n",
        "  ```python\n",
        "  spark.conf.set(\"spark.hadoop.fs.s3a.committer.name\", \"magic\")\n",
        "  spark.conf.set(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\",\n",
        "                \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\")\n",
        "  ```\n",
        "- **Set Hadoop Configs**:\n",
        "  ```python\n",
        "  spark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. File System Consistency**\n",
        "#### **Cause**:\n",
        "- **Eventual Consistency**: Cloud storage (e.g., S3) delays visibility of written files.\n",
        "- **Partial Writes**: Job crashes mid-write, leaving incomplete files.\n",
        "\n",
        "#### **Fix**:\n",
        "- **Use Transactional Formats**:\n",
        "  ```python\n",
        "  df.write.format(\"delta\").save(\"output_path\")  # Delta Lake\n",
        "  df.write.format(\"iceberg\").save(\"output_path\")  # Iceberg\n",
        "  ```\n",
        "- **Check for `_SUCCESS` Files**:\n",
        "  ```bash\n",
        "  hdfs dfs -ls /output_path/_SUCCESS  # Should exist if job succeeded\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Code-Level Issues**\n",
        "#### **Cause**:\n",
        "- **Custom Sinks/UDFs**: Manual file operations that don‚Äôt close resources.\n",
        "- **Shuffle Spill**: Disk spills during shuffles not cleaned up.\n",
        "\n",
        "#### **Fix**:\n",
        "- **Audit Custom Code**:\n",
        "  ```python\n",
        "  # Bad: Missing close()\n",
        "  with open(\"local_file.txt\", \"w\") as f:\n",
        "      f.write(\"data\")\n",
        "\n",
        "  # Good: Explicit close\n",
        "  f = open(\"local_file.txt\", \"w\")\n",
        "  try:\n",
        "      f.write(\"data\")\n",
        "  finally:\n",
        "      f.close()\n",
        "  ```\n",
        "- **Monitor Shuffle**:\n",
        "  ```python\n",
        "  spark.conf.set(\"spark.executor.extraJavaOptions\",\n",
        "                \"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Recovery Steps**\n",
        "If the job already failed:\n",
        "1. **Delete Incomplete Output**:\n",
        "   ```bash\n",
        "   hdfs dfs -rm -r /output_path  # HDFS\n",
        "   aws s3 rm --recursive s3://bucket/output_path/  # S3\n",
        "   ```\n",
        "2. **Re-run with Checks**:\n",
        "   - Enable Spark UI (`4040` port) to monitor file writes.\n",
        "   - Check executor logs for `IOException` or `FileNotFoundException`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Preventive Best Practices**\n",
        "| **Issue**               | **Prevention**                                                                 |\n",
        "|-------------------------|-------------------------------------------------------------------------------|\n",
        "| Resource Leaks          | Use `try-finally`, avoid `kill -9`.                                           |\n",
        "| Cloud Storage Writes    | Enable S3A committers or use Delta Lake/Iceberg.                              |\n",
        "| Shuffle Stability       | Tune `spark.shuffle.spill` and executor memory.                               |\n",
        "| File Permissions        | Pre-create output paths with correct permissions.                             |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Configuration for S3**\n",
        "```python\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") \\\n",
        "    .config(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\",\n",
        "            \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") \\\n",
        "    .getOrCreate()\n",
        "```\n"
      ],
      "metadata": {
        "id": "XkmuGIYe0LhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10 - Your Spark job shows too many tasks for small data. What do you check?\n",
        "\n",
        "When a Spark job generates **too many tasks for small data**, it leads to unnecessary overhead and slower performance due to task scheduling and coordination costs. Here‚Äôs how to diagnose and fix the issue:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Check Partition Count**\n",
        " **Why?**  \n",
        "Tasks correspond to partitions. Small data split into too many partitions causes inefficiency.  \n",
        " **How?**  \n",
        "```python\n",
        "df.rdd.getNumPartitions()  # Check current partition count\n",
        "```\n",
        "- **Expected**: For small data (e.g., <1GB), partitions should ideally match core count (e.g., 2‚Äì4x executors √ó cores).  \n",
        "- **Fix**:  \n",
        "  ```python\n",
        "  df = df.coalesce(4)  # Reduce partitions (no shuffle)\n",
        "  # Or repartition (with shuffle if needed):\n",
        "  df = df.repartition(8)  \n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **2. Review Default Parallelism**\n",
        " **Why?**  \n",
        "`spark.default.parallelism` controls partition count for RDDs/shuffles.  \n",
        " **How?**  \n",
        "```python\n",
        "spark.conf.get(\"spark.default.parallelism\")  # Default: often 200 (too high for small data)\n",
        "```\n",
        "- **Fix**:  \n",
        "  ```python\n",
        "  spark.conf.set(\"spark.default.parallelism\", \"16\")  # Set to ~2-4x total cores\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Inspect Input File Splitting**\n",
        " **Why?**  \n",
        "Small files (e.g., 100x 1MB files) generate 1 partition per file.  \n",
        " **How?**  \n",
        "```python\n",
        "df.inputFiles()  # List input files (check count/size)\n",
        "```\n",
        "- **Fix**:  \n",
        "  - **Merge files** upstream (e.g., `df.repartition(4).write.parquet()`).  \n",
        "  - Use **`spark.sql.files.maxPartitionBytes`** to control split size:  \n",
        "    ```python\n",
        "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128MB\")  # Default: 128MB\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Examine Shuffle Partitions**\n",
        " **Why?**  \n",
        "Shuffles (joins/aggregations) use `spark.sql.shuffle.partitions` (default: 200).  \n",
        " **How?**  \n",
        "```python\n",
        "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
        "```\n",
        "- **Fix**:  \n",
        "  ```python\n",
        "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")  # Match data size\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Check Adaptive Query Execution (AQE)**\n",
        " **Why?**  \n",
        "AQE can coalesce small partitions post-shuffle (Spark 3.0+).  \n",
        " **How?**  \n",
        "```python\n",
        "spark.conf.get(\"spark.sql.adaptive.enabled\")  # Should be 'true'\n",
        "spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\")  # Should be 'true'\n",
        "```\n",
        "- **Fix**: Ensure AQE is enabled:  \n",
        "  ```python\n",
        "  spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "  spark.conf.set(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"16MB\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Look for Skewed Operations**\n",
        " **Why?**  \n",
        "Skewed data can create uneven partitions, forcing more tasks.  \n",
        " **How?**  \n",
        "```python\n",
        "df.groupBy(\"key\").count().orderBy(\"count\").show()  # Check key distribution\n",
        "```\n",
        "- **Fix**:  \n",
        "  - **Salting**: Add random prefixes to skewed keys.  \n",
        "  - **Repartition**: Balance data manually.  \n",
        "\n",
        "---\n",
        "\n",
        " **7. Review Code for Unnecessary Splits**\n",
        " **Why?**  \n",
        "Operations like `explode()` or `flatMap` multiply rows/partitions.  \n",
        " **Fix**:  \n",
        "- Filter early to reduce data volume:  \n",
        "  ```python\n",
        "  df.filter(\"col > 100\").explode(\"array_col\")  # Instead of exploding first\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **Summary of Key Configs**\n",
        "| **Config**                                  | **Purpose**                                  | **Recommended Value for Small Data** |\n",
        "|---------------------------------------------|---------------------------------------------|--------------------------------------|\n",
        "| `spark.default.parallelism`                 | Default partition count for RDDs.           | 2‚Äì4x total cores (e.g., 16)          |\n",
        "| `spark.sql.shuffle.partitions`              | Partitions during shuffles.                 | 16‚Äì32                                |\n",
        "| `spark.sql.files.maxPartitionBytes`         | Max split size for input files.             | 64‚Äì128MB                             |\n",
        "| `spark.sql.adaptive.enabled`                | Enable AQE to coalesce partitions.          | `true`                               |\n",
        "\n",
        "---\n",
        "\n",
        " **Example Workflow**\n",
        "```python\n",
        "# 1. Read data and check partitions\n",
        "df = spark.read.parquet(\"small_data/\")\n",
        "print(\"Partitions before:\", df.rdd.getNumPartitions())  # e.g., 200\n",
        "\n",
        "# 2. Reduce partitions and tune configs\n",
        "df = df.coalesce(4)\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "\n",
        "# 3. Verify after operation\n",
        "df.groupBy(\"key\").count().show()\n",
        "print(\"Partitions after:\", df.rdd.getNumPartitions())  # e.g., 8\n",
        "```\n",
        "\n",
        "**Key Takeaway**: The goal is to **align partition count with data size and cluster resources**. Fewer partitions reduce overhead, but too few can cause underutilization."
      ],
      "metadata": {
        "id": "y4Z4r5_o4wsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. You're integrating Spark with a JDBC source. How do you optimize?\n",
        "\n",
        "Optimizing Spark's integration with a JDBC source requires balancing **performance**, **database load**, and **resource efficiency**. Here‚Äôs a step-by-step guide to maximize throughput while minimizing strain on the source database:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Partitioning for Parallel Reads**\n",
        " **Why?**  \n",
        "Avoid single-threaded reads by splitting the query into parallel tasks.\n",
        " **How?**  \n",
        "Use **partitioning columns**, **predicates**, or **custom bounds**:\n",
        "```python\n",
        " Method 1: Partition by column (numeric/date)\n",
        "df = spark.read.jdbc(\n",
        "    url=\"jdbc:postgresql://host/db\",\n",
        "    table=\"orders\",\n",
        "    column=\"order_id\",   Partition column\n",
        "    lowerBound=1,\n",
        "    upperBound=100000,\n",
        "    numPartitions=10,   Parallel tasks\n",
        "    properties={\"user\": \"user\", \"password\": \"pass\"}\n",
        ")\n",
        "\n",
        " Method 2: Custom predicates (e.g., date ranges)\n",
        "predicates = [f\"date >= '2023-01-01' AND date < '2023-02-01'\",\n",
        "              f\"date >= '2023-02-01' AND date < '2023-03-01'\"]\n",
        "df = spark.read.jdbc(url, table, predicates=predicates)\n",
        "```\n",
        "**Key Settings**:\n",
        "- `numPartitions`: Match to executor cores (e.g., 10‚Äì100).\n",
        "- Choose a **high-cardinality column** (e.g., IDs, dates) to avoid skew.\n",
        "\n",
        "---\n",
        "\n",
        " **2. Tuning Fetch Size**\n",
        " **Why?**  \n",
        "Control how many rows are fetched per round-trip (reduces network calls).\n",
        " **How?**  \n",
        "```python\n",
        "properties = {\n",
        "    \"user\": \"user\",\n",
        "    \"password\": \"pass\",\n",
        "    \"fetchSize\": \"10000\"   Default is usually 10-1000\n",
        "}\n",
        "df = spark.read.jdbc(url, table, properties=properties)\n",
        "```\n",
        "- **Ideal `fetchSize`**: 1,000‚Äì50,000 (tradeoff between memory and latency).\n",
        "\n",
        "---\n",
        "\n",
        " **3. Pushdown Optimization**\n",
        " **Why?**  \n",
        "Filter data at the database level (not in Spark).\n",
        " **How?**  \n",
        "```python\n",
        " Pushdown filters via SQL query (instead of table name)\n",
        "query = \"(SELECT * FROM orders WHERE status = 'shipped') AS tmp\"\n",
        "df = spark.read.jdbc(url, query, properties=properties)\n",
        "\n",
        " Or use Spark's filter pushdown:\n",
        "df.filter(\"status = 'shipped'\").explain()   Check if filter is pushed to JDBC\n",
        "```\n",
        "- **Verify**: In Spark UI, the SQL query should include the filter.\n",
        "\n",
        "---\n",
        "\n",
        " **4. Write Optimization**\n",
        " **Batching Inserts**  \n",
        "```python\n",
        "df.write.jdbc(\n",
        "    url=url,\n",
        "    table=\"target_table\",\n",
        "    mode=\"append\",\n",
        "    properties={\n",
        "        \"user\": \"user\",\n",
        "        \"password\": \"pass\",\n",
        "        \"batchsize\": \"50000\"   Default: 1000\n",
        "    }\n",
        ")\n",
        "```\n",
        "- **`batchsize`**: Increase (e.g., 10k‚Äì100k) to reduce round-trips.\n",
        "\n",
        " **Disable Constraints Temporarily**  \n",
        "For bulk writes, disable indexes/constraints in the database before writing.\n",
        "\n",
        "---\n",
        "\n",
        " **5. Connection Pooling**\n",
        " **Why?**  \n",
        "Reuse connections to avoid overhead.\n",
        " **How?**  \n",
        "```python\n",
        "properties = {\n",
        "    \"user\": \"user\",\n",
        "    \"password\": \"pass\",\n",
        "    \"driver\": \"org.postgresql.Driver\",\n",
        "    \"connectionPool\": \"HikariCP\",   Use connection pool\n",
        "    \"maximumPoolSize\": \"10\"         Match numPartitions\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Avoid OOMs**\n",
        " **Driver Memory**  \n",
        "- Large metadata/result sets can crash the driver.  \n",
        "- **Fix**:  \n",
        "  ```python\n",
        "  spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
        "  ```\n",
        "\n",
        " **Executor Memory**  \n",
        "- Partition data to fit in executor memory:  \n",
        "  ```python\n",
        "  df.repartition(100).write.jdbc(...)   More partitions = less data per task\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **7. Database-Specific Tweaks**\n",
        "| **Database** | **Optimization**                                  |\n",
        "|--------------|--------------------------------------------------|\n",
        "| PostgreSQL   | `SET work_mem = '256MB';` (per-connection memory) |\n",
        "| MySQL        | `useServerPrepStmts=false` (for batch writes)     |\n",
        "| Oracle       | `oracle.jdbc.readTimeout=300000` (timeout adjust) |\n",
        "\n",
        "---\n",
        "\n",
        " **8. Monitoring & Validation**\n",
        "- **Check Spark UI**: Look for skewed partitions in JDBC read stages.\n",
        "- **Database Metrics**: Monitor CPU/query latency during extraction.\n",
        "- **Logs**: Verify pushdown filters in Spark query plans (`df.explain()`).\n",
        "\n",
        "---\n",
        "\n",
        " **Example Configurations**\n",
        " **Optimal Read**\n",
        "```python\n",
        "df = spark.read.jdbc(\n",
        "    url=url,\n",
        "    table=\"large_table\",\n",
        "    column=\"id\",\n",
        "    lowerBound=1,\n",
        "    upperBound=1000000,\n",
        "    numPartitions=50,\n",
        "    properties={\n",
        "        \"user\": \"user\",\n",
        "        \"password\": \"pass\",\n",
        "        \"fetchSize\": \"50000\",\n",
        "        \"driver\": \"org.postgresql.Driver\"\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        " **Optimal Write**\n",
        "```python\n",
        "df.repartition(20).write.jdbc(\n",
        "    url=url,\n",
        "    table=\"target_table\",\n",
        "    mode=\"overwrite\",\n",
        "    properties={\n",
        "        \"user\": \"user\",\n",
        "        \"password\": \"pass\",\n",
        "        \"batchsize\": \"50000\",\n",
        "        \"truncate\": \"true\"   Faster overwrites (database-dependent)\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Takeaways**\n",
        "1. **Partition Smartly**: Use `numPartitions` + bounds/predicates.\n",
        "2. **Minimize Round-Trips**: Tune `fetchSize` and `batchsize`.\n",
        "3. **Offload Work**: Push filters/aggregations to the database.\n",
        "4. **Monitor Resources**: Avoid overwhelming the DB or Spark.\n",
        "5. **Database Tuning**: Adjust DB-side settings (timeouts, memory).\n",
        "\n",
        "By aligning Spark's parallelism with the database's capabilities, you can achieve high throughput without degrading source performance.\n"
      ],
      "metadata": {
        "id": "-2dN9BL17UlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12 - You're working with streaming data. How to handle late-arriving events?\n",
        "Handling late-arriving events in Spark Streaming or Structured Streaming requires a combination of **time-based buffering**, **state management**, and **grace period policies**. Here‚Äôs a comprehensive approach:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Define Watermarks**\n",
        "**What it does**:  \n",
        "Tracks event-time progress and bounds how late data can arrive before being ignored.  \n",
        "\n",
        "**How to implement**:  \n",
        "```python\n",
        "from pyspark.sql.functions import window, col\n",
        "\n",
        "stream = spark.readStream \\\n",
        "    .schema(schema) \\\n",
        "    .json(\"path/to/stream\") \\\n",
        "    .withWatermark(\"event_time\", \"10 minutes\")   Late threshold = 10 mins\n",
        "```\n",
        "- **Key**: Events older than `10 minutes` past the latest watermark are dropped.  \n",
        "- **Tradeoff**: Larger values increase state size but accommodate more delays.\n",
        "\n",
        "---\n",
        "\n",
        " **2. Use Windowed Aggregations with Grace Period**\n",
        "**What it does**:  \n",
        "Allows late data to update windows within a grace period.  \n",
        "\n",
        "**Example**:  \n",
        "```python\n",
        "windowed_counts = stream \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"5 minutes\", \"1 minute\"),   5-min window, sliding every 1 min\n",
        "        col(\"user_id\")\n",
        "    ) \\\n",
        "    .count() \\\n",
        "    .withWatermark(\"event_time\", \"10 minutes\")   Must match watermark\n",
        "```\n",
        "- **Grace Period**: Late data within `10 minutes` of the window‚Äôs end time can update results.  \n",
        "- **Output Mode**: Use `update` or `complete` (not `append`) to reflect late arrivals.\n",
        "\n",
        "---\n",
        "\n",
        " **3. Handle Late Events in Stateful Operations**\n",
        "**For custom state (e.g., sessionization)**:  \n",
        "Use `mapGroupsWithState` or `flatMapGroupsWithState` to:  \n",
        "- **Timeout stale state**: Discard state after inactivity.  \n",
        "- **Merge late updates**: Re-process if within watermark bounds.  \n",
        "\n",
        "**Example (Scala API)**:  \n",
        "```scala\n",
        "val query = stream\n",
        "  .groupByKey(_.userId)\n",
        "  .mapGroupsWithState(GroupStateTimeout.EventTimeTimeout) {\n",
        "    case (userId, events, state) =>\n",
        "      // Custom logic to handle late events\n",
        "      if (state.hasTimedOut) cleanup()\n",
        "      else updateState()\n",
        "  }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Output Modes for Late Data**\n",
        "| **Mode**     | **Behavior**                                  | **Use Case**                     |\n",
        "|--------------|----------------------------------------------|----------------------------------|\n",
        "| `append`     | Ignores late data post-watermark.            | Simple filtering.                |\n",
        "| `update`     | Updates existing rows for late data.         | Real-time dashboards.            |\n",
        "| `complete`   | Recomputes all results (expensive).          | Batch-like correctness.          |\n",
        "\n",
        "**Example**:  \n",
        "```python\n",
        "query = windowed_counts.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Reprocessing with Sinks**\n",
        "For critical late data:  \n",
        "- **Write raw late events** to a dead-letter queue (e.g., Kafka, Delta Lake).  \n",
        "- **Re-process periodically** using batch jobs.  \n",
        "\n",
        "**Example**:  \n",
        "```python\n",
        " Write late events to Delta Lake\n",
        "late_events = stream.filter(\"event_time < watermark\")\n",
        "late_events.writeStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"path\", \"/path/late_events\") \\\n",
        "    .start()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Tune Checkpointing**\n",
        "**Why**:  \n",
        "Ensures state recovery after failures.  \n",
        "**How**:  \n",
        "```python\n",
        "query = stream.writeStream \\\n",
        "    .option(\"checkpointLocation\", \"/path/checkpoint\") \\\n",
        "    .start()\n",
        "```\n",
        "- **Required** for stateful operations (watermarks, aggregations).\n",
        "\n",
        "---\n",
        "\n",
        " **7. Database Integration (CDC)**\n",
        "For Change Data Capture (CDC) pipelines:  \n",
        "- Use **Debezium** or **AWS DMS** to capture late database updates.  \n",
        "- Merge with streaming data using `MERGE` in Delta Lake/Iceberg.  \n",
        "\n",
        "**Example**:  \n",
        "```sql\n",
        "MERGE INTO target_table t\n",
        "USING late_events_stream s\n",
        "ON t.id = s.id\n",
        "WHEN MATCHED THEN UPDATE SET *\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Configurations**\n",
        "| **Parameter**                     | **Purpose**                                  | **Recommended Value**          |\n",
        "|-----------------------------------|---------------------------------------------|--------------------------------|\n",
        "| `spark.sql.streaming.statefulOperator.checkpointInterval` | Frequency of checkpoints. | `2 minutes` (default)          |\n",
        "| `spark.sql.shuffle.partitions`    | Parallelism for stateful ops.               | 2‚Äì4x cores                     |\n",
        "| `spark.sql.streaming.noDataMicros`| How long to wait for new data.              | `600000000` (10 mins)          |\n",
        "\n",
        "---\n",
        "\n",
        " **Summary of Strategies**\n",
        "1. **Watermarks**: Set thresholds for late data (`withWatermark`).  \n",
        "2. **Grace Periods**: Allow window updates (`groupBy(window)`).  \n",
        "3. **State Timeouts**: Clean up stale state (`mapGroupsWithState`).  \n",
        "4. **Dead-Letter Queues**: Re-process late events offline.  \n",
        "5. **Checkpointing**: Ensure fault tolerance.  \n",
        "\n",
        "**Tradeoffs**:  \n",
        "- **Low Latency**: Smaller watermarks/grace periods (risk dropping data).  \n",
        "- **Correctness**: Larger buffers (higher state size).  \n",
        "\n",
        "By combining these techniques, you can balance real-time responsiveness with data completeness.\n"
      ],
      "metadata": {
        "id": "ZeVqvHIU7of0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q13 - Spark job fails with 'Stage canceled' error. How do you debug?\n",
        "When a Spark job fails with a **\"Stage canceled\"** error, it typically indicates that a stage was intentionally terminated due to an external issue or internal Spark logic. Here‚Äôs a systematic approach to debug and resolve the problem:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Check Spark UI and Logs**\n",
        " **Step 1: Access Spark UI**\n",
        "- Navigate to `http://<driver-node>:4040` (or the configured port) to inspect:\n",
        "  - **Stages Tab**: Look for the canceled stage (marked in red) and check its details.\n",
        "  - **Executors Tab**: Verify if executors are alive or were lost (`ExecutorLostFailure`).\n",
        "  - **Environment Tab**: Review misconfigured settings.\n",
        "\n",
        " **Step 2: Examine Logs**\n",
        "- **Driver Logs**: Search for `Stage cancelled` and preceding warnings/errors.\n",
        "  ```bash\n",
        "  grep -i \"Stage cancelled\" /path/to/spark-driver.log\n",
        "  ```\n",
        "- **Executor Logs**: Check for OOM, network timeouts, or task failures.\n",
        "  ```bash\n",
        "  grep -i \"ERROR\" /path/to/executor.log\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **2. Common Causes and Fixes**\n",
        " **A. Executor Failures**\n",
        "- **Symptoms**: `ExecutorLostFailure`, `Lost executor X on Y`.\n",
        "- **Root Causes**:\n",
        "  - **OOM Errors**: Executors ran out of memory.\n",
        "  - **Node Crashes**: Cluster resource issues (e.g., preemption, hardware failure).\n",
        "- **Fixes**:\n",
        "  - Increase executor memory:\n",
        "    ```python\n",
        "    spark-submit --executor-memory 8G ...\n",
        "    ```\n",
        "  - Tune garbage collection:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=200\")\n",
        "    ```\n",
        "\n",
        " **B. Task Failures Exceeding Threshold**\n",
        "- **Symptoms**: `Task failed 4 times (most recent failure: ...)`.\n",
        "- **Root Cause**: Repeated task failures trigger stage cancellation (default: 4 retries).\n",
        "- **Fixes**:\n",
        "  - Increase retry limit:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.task.maxFailures\", \"8\")\n",
        "    ```\n",
        "  - Debug task errors (e.g., data skew, UDF bugs):\n",
        "    ```python\n",
        "    df.sample(0.1).collect()   Test on a sample\n",
        "    ```\n",
        "\n",
        " **C. Manual Cancellation**\n",
        "- **Symptoms**: `Cancelled because of attempt failure on another executor`.\n",
        "- **Root Cause**:\n",
        "  - User-initiated cancellation (e.g., `sparkContext.cancelJob()`).\n",
        "  - Upstream stage failed, forcing dependent stages to cancel.\n",
        "- **Fix**: Check job submission scripts/notebooks for accidental cancellations.\n",
        "\n",
        " **D. Dynamic Resource Scaling**\n",
        "- **Symptoms**: `Stage cancelled because BarrierJob failed`.\n",
        "- **Root Cause**: Cluster managers (YARN/K8s) reclaimed resources.\n",
        "- **Fixes**:\n",
        "  - Disable dynamic allocation if unstable:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
        "    ```\n",
        "  - Allocate fixed resources:\n",
        "    ```python\n",
        "    spark-submit --num-executors 10 ...\n",
        "    ```\n",
        "\n",
        " **E. Shuffle Service Issues**\n",
        "- **Symptoms**: `ShuffleBlockFetcherIterator: Failed to get shuffle data`.\n",
        "- **Root Cause**: Shuffle service crashes or network partitions.\n",
        "- **Fixes**:\n",
        "  - Restart shuffle service on worker nodes.\n",
        "  - Increase shuffle timeout:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.shuffle.io.retryWait\", \"60s\")\n",
        "    spark.conf.set(\"spark.shuffle.io.maxRetries\", \"10\")\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Advanced Debugging**\n",
        " **Step 1: Reproduce with Smaller Data**\n",
        "- Test on a subset to isolate the issue:\n",
        "  ```python\n",
        "  df.limit(1000).write.parquet(\"...\")   Check if cancellation persists\n",
        "  ```\n",
        "\n",
        " **Step 2: Enable Debug Logging**\n",
        "- Add verbose logging to pinpoint failures:\n",
        "  ```python\n",
        "  spark.sparkContext.setLogLevel(\"DEBUG\")\n",
        "  ```\n",
        "\n",
        " **Step 3: Check Data Skew**\n",
        "- Skewed partitions cause long-running tasks that may time out:\n",
        "  ```python\n",
        "  df.groupBy(\"key\").count().orderBy(\"count\").show()\n",
        "  ```\n",
        "  - **Fix**: Salt skewed keys or repartition.\n",
        "\n",
        "---\n",
        "\n",
        " **4. Key Configurations to Review**\n",
        "| **Config**                          | **Purpose**                                  | **Recommended Value**          |\n",
        "|-------------------------------------|---------------------------------------------|--------------------------------|\n",
        "| `spark.task.maxFailures`            | Max task retries before stage cancel.       | `8` (default: `4`)             |\n",
        "| `spark.executor.memory`             | Executor heap size.                         | `8G` (adjust per workload)     |\n",
        "| `spark.shuffle.service.enabled`     | External shuffle service.                   | `true` (for YARN/K8s)          |\n",
        "| `spark.dynamicAllocation.enabled`   | Dynamic executor scaling.                   | `false` (if unstable)          |\n",
        "\n",
        "---\n",
        "\n",
        " **5. Example Fix for OOM-Induced Cancellation**\n",
        "```python\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyApp\") \\\n",
        "    .config(\"spark.executor.memory\", \"8G\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"2G\") \\   For off-heap\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\   Reduce partition size\n",
        "    .getOrCreate()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Summary**\n",
        "1. **Inspect Logs/UI**: Identify executor failures, task errors, or manual cancellations.\n",
        "2. **Tune Resources**: Increase memory, retries, or disable dynamic allocation.\n",
        "3. **Check Data**: Address skew or corrupt records.\n",
        "4. **Review Configs**: Adjust shuffle/timeout settings.\n",
        "\n",
        "**Pro Tip**: If the issue persists, enable **Spark event logging** (`spark.eventLog.enabled=true`) and analyze the history server post-failure.\n"
      ],
      "metadata": {
        "id": "lZIeouoe7w3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q14 - Output records have strange characters. What might be wrong?\n",
        "When your Spark job outputs records with **strange characters** (e.g., `ÔøΩ`, `√É¬©`, `\\\\\\\\x00`, or garbled text), it typically indicates an **encoding, serialization, or data corruption issue**. Here‚Äôs how to diagnose and fix the problem:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Check Source Data Encoding**\n",
        " **Issue**:  \n",
        "The input file/database uses an encoding (e.g., `UTF-8`, `ISO-8859-1`) that wasn‚Äôt correctly interpreted.  \n",
        " **Fix**:  \n",
        "Explicitly specify the encoding when reading:  \n",
        "```python\n",
        " For text files:\n",
        "df = spark.read.text(\"path/to/file.txt\", encoding=\"UTF-8\")\n",
        "\n",
        " For CSV/JSON:\n",
        "df = spark.read.option(\"encoding\", \"UTF-8\").csv(\"path/to/file.csv\")\n",
        "```\n",
        "- **Common Encodings**: `UTF-8` (default), `ISO-8859-1`, `Windows-1252`.  \n",
        "- **Check File Encoding**:  \n",
        "  ```bash\n",
        "  file -i input.txt   Linux/macOS\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **2. Binary/Corrupt Data in Text Files**\n",
        " **Issue**:  \n",
        "The file contains binary data (e.g., BOM headers, non-text bytes) or is corrupted.  \n",
        " **Fix**:  \n",
        "- **Skip BOM Headers** (for UTF-8 with BOM):  \n",
        "  ```python\n",
        "  df = spark.read.option(\"encoding\", \"UTF-8\").option(\"charset\", \"UTF-8\").csv(\"file.csv\")\n",
        "  ```\n",
        "- **Filter Out Null Bytes**:  \n",
        "  ```python\n",
        "  from pyspark.sql.functions import col\n",
        "  df = df.filter(~col(\"value\").contains(\"\\x00\"))\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Serialization Issues**\n",
        " **Issue**:  \n",
        "Data was serialized/deserialized incorrectly (e.g., Parquet/ORC schema mismatch).  \n",
        " **Fix**:  \n",
        "- **For Parquet/ORC**:  \n",
        "  ```python\n",
        "  df = spark.read.parquet(\"path.parquet\")   Ensure schema matches\n",
        "  ```\n",
        "- **For Kafka/Avro**:  \n",
        "  ```python\n",
        "  df = spark.read.format(\"kafka\") \\\n",
        "       .option(\"kafka.bootstrap.servers\", \"host:9092\") \\\n",
        "       .option(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\") \\\n",
        "       .load()\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Character Escaping in CSV/JSON**\n",
        " **Issue**:  \n",
        "Special characters (e.g., `\"`, `\\n`) are improperly escaped.  \n",
        " **Fix**:  \n",
        "- **CSV**:  \n",
        "  ```python\n",
        "  df = spark.read.option(\"escape\", \"\\\\\").option(\"quote\", \"\\\"\").csv(\"file.csv\")\n",
        "  ```\n",
        "- **JSON**:  \n",
        "  ```python\n",
        "  df = spark.read.option(\"multiLine\", \"true\").json(\"file.json\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Database Source Issues**\n",
        " **Issue**:  \n",
        "Database collation/encoding (e.g., `latin1` vs. `UTF-8`) mismatches Spark‚Äôs expectations.  \n",
        " **Fix**:  \n",
        "- **JDBC Read**:  \n",
        "  ```python\n",
        "  df = spark.read.jdbc(\n",
        "      url=\"jdbc:mysql://host/db\",\n",
        "      table=\"table\",\n",
        "      properties={\"useUnicode\": \"true\", \"characterEncoding\": \"UTF-8\"}\n",
        "  )\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Debugging Steps**\n",
        "1. **Inspect Raw Data**:  \n",
        "   ```python\n",
        "   df.select(\"problem_column\").show(truncate=False)   Show full content\n",
        "   ```\n",
        "2. **Check Hex Values**:  \n",
        "   ```python\n",
        "   from pyspark.sql.functions import hex\n",
        "   df.select(hex(\"problem_column\")).show()\n",
        "   ```\n",
        "3. **Reproduce Locally**:  \n",
        "   Read a sample file in Python to isolate Spark:  \n",
        "   ```python\n",
        "   with open(\"file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "       print(f.read())\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        " **7. Common Scenarios & Fixes**\n",
        "| **Symptom**               | **Likely Cause**               | **Solution**                              |\n",
        "|---------------------------|--------------------------------|-------------------------------------------|\n",
        "| `√É¬©` instead of `√©`       | UTF-8 misinterpreted as Latin1 | Specify `encoding=\"UTF-8\"`                |\n",
        "| `ÔøΩ` (replacement chars)   | Invalid UTF-8 bytes            | Use `encoding=\"ISO-8859-1\"` or clean data |\n",
        "| `\\\\\\\\x00`                 | Null bytes in text             | Filter or replace nulls                   |\n",
        "| Garbled binary            | Wrong file format              | Use binary readers (e.g., `spark.read.format(\"binaryFile\")`) |\n",
        "\n",
        "---\n",
        "\n",
        " **Example Fix for UTF-8 Misinterpretation**\n",
        "```python\n",
        " Read CSV with explicit UTF-8 encoding and escape rules\n",
        "df = spark.read.option(\"encoding\", \"UTF-8\") \\\n",
        "     .option(\"escape\", \"\\\\\") \\\n",
        "     .csv(\"input.csv\")\n",
        "\n",
        " Write with same encoding\n",
        "df.write.option(\"encoding\", \"UTF-8\").parquet(\"output.parquet\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Takeaways**\n",
        "1. **Always specify encoding** when reading text files.  \n",
        "2. **Validate source data** for corruption/binary content.  \n",
        "3. **Match serialization formats** (e.g., Parquet schemas, Kafka deserializers).  \n",
        "4. **Debug column-wise** using `show()` or `hex()`.  \n",
        "\n",
        "**Pro Tip**: If the issue persists, use `spark.read.format(\"binaryFile\")` to inspect raw bytes before parsing.\n"
      ],
      "metadata": {
        "id": "pyjimvFW8Dqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q15 - Stakeholders want data lineage for each pipeline. What‚Äôs your approach?\n",
        "To provide **data lineage** for each pipeline, you need to track the **origin, transformations, and dependencies** of data across its lifecycle. Here‚Äôs a structured approach to implement lineage tracking in Spark-based pipelines:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Automated Lineage Capture**\n",
        " **A. Built-in Spark Lineage (Logical Plans)**\n",
        "Spark‚Äôs query plans inherently capture lineage at the transformation level. Extract it programmatically:\n",
        "```python\n",
        " Get logical plan as JSON\n",
        "df = spark.table(\"source\").filter(\"id > 100\").groupBy(\"category\").count()\n",
        "lineage_json = df._jdf.queryExecution().logical().toJSON()\n",
        "```\n",
        "- **Pros**: No extra tools needed.\n",
        "- **Cons**: Low-level; requires parsing JSON for business context.\n",
        "\n",
        " **B. OpenLineage Integration**\n",
        "Use OpenLineage (open standard) with Spark listeners:\n",
        "```python\n",
        "spark.sparkContext._jsc.sc().addSparkListener(LineageTrackingListener())\n",
        "```\n",
        "- **Tools**: Marquez, Amundsen, or Egeria.\n",
        "- **Output**: Tracks inputs ‚Üí transformations ‚Üí outputs as a DAG.\n",
        "\n",
        "---\n",
        "\n",
        " **2. Metadata Tagging**\n",
        "Embed lineage metadata directly in output files/databases:\n",
        "```python\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df_with_lineage = df.withColumn(\"_lineage\", lit({\n",
        "    \"source\": \"s3://source-data/2023/01/01\",\n",
        "    \"transformations\": [\"filter_id>100\", \"groupBy_category\"],\n",
        "    \"generated_at\": \"2023-01-01T12:00:00Z\"\n",
        "}))\n",
        "\n",
        "df_with_lineage.write.parquet(\"s3://output-data/\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Pipeline Orchestration Tools**\n",
        "Integrate with orchestration platforms that natively track lineage:\n",
        "- **Airflow**: Use `Dataset` and `Lineage` decorators.\n",
        "  ```python\n",
        "  from airflow.lineage import Dataset\n",
        "  @task(outlets=[Dataset(\"s3://output-data\")])\n",
        "  def run_spark_job():\n",
        "      spark_job()\n",
        "  ```\n",
        "- **Databricks**: Unity Catalog tracks lineage across notebooks/jobs.\n",
        "- **Azure Purview/AWS Glue**: Automatically scans Spark jobs for lineage.\n",
        "\n",
        "---\n",
        "\n",
        " **4. Custom Lineage Tracking**\n",
        "For granular control, log lineage to a metadata store (e.g., PostgreSQL, Neo4j):\n",
        "```python\n",
        "def log_lineage(spark, job_id, inputs, outputs, transformations):\n",
        "    lineage_data = {\n",
        "        \"job_id\": job_id,\n",
        "        \"inputs\": inputs,   e.g., [\"s3://raw/users\", \"s3://raw/orders\"]\n",
        "        \"transformations\": transformations,   e.g., [\"join\", \"aggregation\"]\n",
        "        \"outputs\": outputs,   e.g., \"s3://processed/reports\"\n",
        "        \"timestamp\": datetime.utcnow()\n",
        "    }\n",
        "     Write to metadata DB\n",
        "    spark.createDataFrame([lineage_data]).write.jdbc(...)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Visualization & Governance**\n",
        "- **Visualization Tools**:  \n",
        "  - **Amundsen**: LinkedIn‚Äôs tool for data discovery + lineage.  \n",
        "  - **DataHub**: Metadata framework with lineage graphs.  \n",
        "- **Governance**:  \n",
        "  - Tag sensitive columns (e.g., PII) in lineage metadata.  \n",
        "  - Enforce retention policies based on lineage.\n",
        "\n",
        "---\n",
        "\n",
        " **6. Example End-to-End Workflow**\n",
        "1. **Capture**: Use Spark listeners to log lineage during job execution.  \n",
        "2. **Store**: Write lineage metadata to a searchable store (e.g., Elasticsearch).  \n",
        "3. **Visualize**: Render dependencies in tools like Tableau/PowerBI.  \n",
        "4. **Alert**: Notify stakeholders of upstream changes (e.g., schema drift).  \n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Source: s3://raw/users] --> B[Spark Job: Clean & Join]\n",
        "    C[Source: s3://raw/orders] --> B\n",
        "    B --> D[Output: s3://processed/reports]\n",
        "    B --> E[Lineage DB]\n",
        "    E --> F[Amundsen UI]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Considerations**\n",
        "| **Requirement**       | **Solution**                          | **Tools**                          |\n",
        "|-----------------------|---------------------------------------|------------------------------------|\n",
        "| Lightweight           | Spark logical plans + metadata tags   | OpenLineage, Custom JSON           |\n",
        "| Enterprise-grade      | Integrated governance platforms       | Databricks Unity, Azure Purview    |\n",
        "| Visualization         | Graph databases + UI                  | Amundsen, DataHub, Neo4j           |\n",
        "| Compliance            | PII tagging + audit logs              | Collibra, Alation                  |\n",
        "\n",
        "---\n",
        "\n",
        " **Best Practices**\n",
        "1. **Standardize Metadata**: Use common fields (e.g., `source`, `owner`, `last_updated`).  \n",
        "2. **Automate**: Hook lineage capture into CI/CD pipelines.  \n",
        "3. **Validate**: Regularly check lineage accuracy with sample audits.  \n",
        "\n",
        "**Example Stakeholder Report**:\n",
        "```\n",
        "Data Asset: s3://processed/reports\n",
        "- Sources: s3://raw/users, s3://raw/orders\n",
        "- Transformations: join(user_id), agg(sum(revenue))\n",
        "- Last Updated: 2023-01-01\n",
        "- Owner: analytics-team@company.com\n",
        "```\n",
        "\n",
        "By combining automation, metadata management, and visualization, you can provide transparent, auditable lineage tailored to stakeholder needs.\n"
      ],
      "metadata": {
        "id": "MCNqoSfH8P4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q16 - A pipeline failed due to special characters in column names. How do you handle?\n",
        "When a pipeline fails due to **special characters in column names** (e.g., spaces, hyphens, emojis, or symbols like `@`, ``, `()`), follow this systematic approach to handle and prevent the issue:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Identify Problematic Columns**\n",
        "First, detect which columns contain special characters:\n",
        "```python\n",
        " Check for non-alphanumeric characters in column names\n",
        "problematic_columns = [col for col in df.columns if not col.isidentifier()]\n",
        "print(\"Invalid columns:\", problematic_columns)\n",
        "```\n",
        "**Example Output**:  \n",
        "`['user-id', 'first name', 'amount($)']`\n",
        "\n",
        "---\n",
        "\n",
        " **2. Clean Column Names Automatically**\n",
        "Replace or remove special characters to comply with Spark's naming rules (alphanumeric + underscore):  \n",
        "```python\n",
        "from pyspark.sql.functions import col\n",
        "import re\n",
        "\n",
        " Clean all column names\n",
        "def clean_column(name):\n",
        "    return re.sub(r'[^a-zA-Z0-9_]', '_', name)   Replace special chars with _\n",
        "\n",
        " Apply to DataFrame\n",
        "df_clean = df.select([col(c).alias(clean_column(c)) for c in df.columns])\n",
        "```\n",
        "**Result**:  \n",
        "- `user-id` ‚Üí `user_id`  \n",
        "- `first name` ‚Üí `first_name`  \n",
        "- `amount($)` ‚Üí `amount_`  \n",
        "\n",
        "**Alternative**: Use `trim` or `regexp_replace` for specific cases:\n",
        "```python\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "df = df.withColumnRenamed(\"first name\", \"first_name\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Handle Reserved Keywords**\n",
        "Some column names (e.g., `order`, `timestamp`) conflict with SQL reserved keywords. Escape them with backticks:  \n",
        "```python\n",
        " For querying reserved keywords\n",
        "df.createOrReplaceTempView(\"temp_table\")\n",
        "clean_df = spark.sql(\"SELECT `order`, `timestamp` FROM temp_table\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Prevent Future Issues**\n",
        " **A. Schema Enforcement**\n",
        "Validate column names when reading data:\n",
        "```python\n",
        " Read data and validate columns\n",
        "df = spark.read.csv(\"path/to/file.csv\")\n",
        "invalid_cols = [c for c in df.columns if not c.isidentifier()]\n",
        "if invalid_cols:\n",
        "    raise ValueError(f\"Invalid columns: {invalid_cols}\")\n",
        "```\n",
        "\n",
        " **B. Preprocessing Script**\n",
        "Add a preprocessing step to clean column names before pipeline execution:\n",
        "```python\n",
        "def sanitize_columns(df):\n",
        "    return df.toDF(*[clean_column(c) for c in df.columns])\n",
        "\n",
        "df = spark.read.csv(\"path/to/file.csv\")\n",
        "df = sanitize_columns(df)\n",
        "```\n",
        "\n",
        " **C. Use Delta Lake/Iceberg**\n",
        "These table formats enforce stricter schema rules and can reject invalid names upfront.\n",
        "\n",
        "---\n",
        "\n",
        " **5. Special Cases**\n",
        " **Unicode/Emoji Characters**\n",
        "Replace non-ASCII characters (e.g., `√º`, `‚Üí`, `üòä`) if they cause issues:\n",
        "```python\n",
        "def remove_unicode(name):\n",
        "    return name.encode('ascii', 'ignore').decode('ascii')   Drops non-ASCII\n",
        "```\n",
        "\n",
        " **Case Sensitivity**\n",
        "Standardize to lowercase/uppercase to avoid conflicts:\n",
        "```python\n",
        "df = df.toDF(*[c.lower() for c in df.columns])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **6. Example End-to-End Fix**\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "import re\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        " Step 1: Read raw data\n",
        "df = spark.read.csv(\"data_with_special_chars.csv\", header=True)\n",
        "\n",
        " Step 2: Clean column names\n",
        "df_clean = df.toDF(*[re.sub(r'[^a-zA-Z0-9_]', '_', c) for c in df.columns])\n",
        "\n",
        " Step 3: Write sanitized data\n",
        "df_clean.write.parquet(\"cleaned_data.parquet\")\n",
        "```\n",
        "\n",
        "**Output Schema Before/After**:\n",
        "```\n",
        "Before: user-id, first name, amount($)\n",
        "After:  user_id, first_name, amount_\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Takeaways**\n",
        "| **Scenario**               | **Solution**                              | **Tools/Methods**                     |\n",
        "|----------------------------|------------------------------------------|---------------------------------------|\n",
        "| Spaces/symbols             | Replace with `_`                         | `regexp_replace`, `alias`             |\n",
        "| Reserved keywords          | Escape with backticks (` `` `)           | Spark SQL                             |\n",
        "| Unicode/emoji              | Remove or transliterate                  | `encode('ascii', 'ignore')`           |\n",
        "| Prevention                 | Schema validation + preprocessing        | Delta Lake, sanitize_columns()        |\n",
        "\n",
        "**Best Practices**:\n",
        "1. **Standardize early**: Clean column names at ingestion.\n",
        "2. **Log changes**: Track column name mappings for auditing.\n",
        "3. **Test rigorously**: Validate with unit tests for edge cases (e.g., `null`, empty strings).\n",
        "\n",
        "By automating column name sanitization and enforcing naming conventions, you can avoid pipeline failures and improve maintainability.\n"
      ],
      "metadata": {
        "id": "0Dzd-UC28SQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q17 - You want to validate data freshness in S3. How do you check?\n",
        "\n",
        "Validating data freshness in S3 ensures your pipeline processes up-to-date data. Here‚Äôs how to check it programmatically and automate monitoring:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Check File Timestamps in S3**\n",
        " **A. Using AWS CLI**\n",
        "```bash\n",
        " Get last modified time of a specific file\n",
        "aws s3 ls s3://your-bucket/path/to/file.csv --recursive | awk '{print $1, $2, $4}'\n",
        "\n",
        " Check latest file in a prefix\n",
        "aws s3 ls s3://your-bucket/path/ --recursive | sort | tail -n 1\n",
        "```\n",
        "**Output**:  \n",
        "`2023-10-01 12:30:00 data/2023-10-01/file.csv`\n",
        "\n",
        " **B. Using Python (boto3)**\n",
        "```python\n",
        "import boto3\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "response = s3.list_objects_v2(Bucket='your-bucket', Prefix='path/to/data/')\n",
        "\n",
        " Get latest file timestamp\n",
        "latest_file = max(response['Contents'], key=lambda x: x['LastModified'])\n",
        "freshness = (datetime.now(timezone.utc) - latest_file['LastModified']).total_seconds() / 3600\n",
        "print(f\"Data is {freshness:.2f} hours old\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **2. Validate Metadata in Spark**\n",
        "Read S3 file metadata directly in PySpark:\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import input_file_name, max as spark_max\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        " Read data and extract file timestamps\n",
        "df = spark.read.parquet(\"s3a://your-bucket/path/\")\n",
        "df_with_metadata = df.withColumn(\"file_path\", input_file_name())\n",
        "\n",
        " Get latest file timestamp (requires Hadoop FileSystem)\n",
        "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
        "    spark._jsc.hadoopConfiguration()\n",
        ")\n",
        "latest_timestamp = df_with_metadata.select(\"file_path\").distinct().rdd.map(\n",
        "    lambda x: fs.getFileStatus(\n",
        "        spark._jvm.org.apache.hadoop.fs.Path(x.file_path)\n",
        "    ).getModificationTime() / 1000   Convert ms to seconds\n",
        ").max()\n",
        "\n",
        "from datetime import datetime\n",
        "print(f\"Latest data timestamp: {datetime.fromtimestamp(latest_timestamp)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Automate Freshness Checks**\n",
        " **A. Airflow Sensor**\n",
        "```python\n",
        "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n",
        "from datetime import timedelta\n",
        "\n",
        "s3_sensor = S3KeySensor(\n",
        "    task_id=\"check_data_freshness\",\n",
        "    bucket_key=\"path/to/latest_file.parquet\",\n",
        "    bucket_name=\"your-bucket\",\n",
        "    aws_conn_id=\"aws_default\",\n",
        "    timeout=60 * 60 * 24,   Timeout after 1 day\n",
        "    poke_interval=60 * 60,   Check hourly\n",
        "    mode=\"reschedule\",\n",
        ")\n",
        "```\n",
        "\n",
        " **B. Lambda + CloudWatch Alerts**\n",
        "1. **Lambda Function** (checks freshness every hour):\n",
        "```python\n",
        "import boto3\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    s3 = boto3.client('s3')\n",
        "    response = s3.list_objects_v2(Bucket='your-bucket', Prefix='data/')\n",
        "    \n",
        "    latest_file = max(response['Contents'], key=lambda x: x['LastModified'])\n",
        "    freshness_hours = (datetime.now(timezone.utc) - latest_file['LastModified']).total_seconds() / 3600\n",
        "    \n",
        "    if freshness_hours > 24:\n",
        "        sns = boto3.client('sns')\n",
        "        sns.publish(\n",
        "            TopicArn='arn:aws:sns:us-east-1:1234567890:alerts',\n",
        "            Message=f\"Data in s3://your-bucket/data/ is stale ({freshness_hours:.1f} hours old)\"\n",
        "        )\n",
        "```\n",
        "2. **CloudWatch Trigger**: Schedule to run hourly.\n",
        "\n",
        "---\n",
        "\n",
        " **4. Data Freshness Dashboard**\n",
        "Use AWS Athena + QuickSight to visualize freshness:\n",
        "```sql\n",
        "-- Athena query to track file timestamps\n",
        "SELECT\n",
        "    split(\"$path\", '/')[-1] as file_name,\n",
        "    date_parse(\n",
        "        substr(\"$path\", position('dt=' IN \"$path\") + 3, 10),\n",
        "        '%Y-%m-%d'\n",
        "    ) as partition_date,\n",
        "    from_unixtime(max(\"$file_modified_time\") / 1000) as latest_update\n",
        "FROM\n",
        "    your_table\n",
        "GROUP BY\n",
        "    1, 2\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Key Checks for Data Freshness**\n",
        "| **Check**                | **Method**                                                                 | **Alert Threshold**          |\n",
        "|--------------------------|---------------------------------------------------------------------------|------------------------------|\n",
        "| File timestamp           | `aws s3 ls` or `boto3.list_objects_v2()`                                 | >1 hour (for real-time)      |\n",
        "| Partition age            | Hive-style paths (`s3://bucket/dt=2023-10-01/`)                          | >24 hours (for daily batches)|\n",
        "| _SUCCESS file            | Check for `_SUCCESS` marker in S3 prefix                                  | Missing after expected time  |\n",
        "| Metadata column          | Extract `last_updated` from data itself (if available)                    | Custom logic                 |\n",
        "\n",
        "---\n",
        "\n",
        " **6. Example Validation in PySpark**\n",
        "```python\n",
        "from pyspark.sql.functions import max as spark_max\n",
        "\n",
        " Assuming a 'timestamp' column exists in the data\n",
        "max_timestamp = spark.read.parquet(\"s3a://your-bucket/path/\") \\\n",
        "    .select(spark_max(\"timestamp\")).first()[0]\n",
        "\n",
        "print(f\"Latest record timestamp: {max_timestamp}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Best Practices**\n",
        "1. **Monitor Partitioned Data**:  \n",
        "   - Validate Hive-style paths (e.g., `s3://bucket/dt=2023-10-01/`).  \n",
        "   - Use AWS Glue Crawlers to keep metastore updated.  \n",
        "2. **Alert on Staleness**:  \n",
        "   - Trigger alerts if no new files arrive within expected intervals.  \n",
        "3. **Combine Metrics**:  \n",
        "   - Cross-check S3 timestamps with data-level timestamps for corruption edge cases.  \n",
        "\n",
        "**Pro Tip**: For critical pipelines, implement **two-phase validation**:  \n",
        "1. Check S3 file timestamps (quick).  \n",
        "2. Verify max timestamp in the data itself (accurate but slower).  \n",
        "\n",
        "This ensures both file delivery and content freshness.\n"
      ],
      "metadata": {
        "id": "mMUSvzis8d7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q18 - You're seeing 'Too many open files' error. What could be wrong?\n",
        "The **\"Too many open files\"** error in Spark indicates that the system‚Äôs file descriptor limit has been exceeded. This is a common issue in data-intensive workloads. Here‚Äôs how to diagnose and resolve it:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Root Causes**\n",
        " **A. System-Level Limits**\n",
        "- **Default limits** are too low (e.g., 1024 descriptors per process on Linux).\n",
        "- **Spark jobs** open many files (especially with thousands of partitions or small files).\n",
        "\n",
        " **B. Resource Leaks**\n",
        "- Unclosed file handles in:\n",
        "  - **Spark operations**: Shuffle files, broadcast variables.\n",
        "  - **Custom code**: UDFs/Java code not closing `InputStream`s/`OutputStream`s.\n",
        "\n",
        " **C. Small File Problem**\n",
        "- Reading/writing **thousands of small files** (e.g., 10,000x 1MB files) strains descriptors.\n",
        "\n",
        "---\n",
        "\n",
        " **2. Immediate Fixes**\n",
        " **Increase File Descriptor Limits**\n",
        "**Linux/MacOS**:\n",
        "```bash\n",
        " Check current limits\n",
        "ulimit -n   User-level\n",
        "cat /proc/sys/fs/file-max   System-wide\n",
        "\n",
        " Temporarily increase (for current session)\n",
        "ulimit -n 65536\n",
        "\n",
        " Permanently increase (add to /etc/security/limits.conf)\n",
        "* soft nofile 65536\n",
        "* hard nofile 65536\n",
        "```\n",
        "**Spark Config** (if running as a service):\n",
        "```bash\n",
        " Add to spark-env.sh\n",
        "SPARK_DAEMON_ULIMIT=\"nofile=65536\"\n",
        "```\n",
        "\n",
        " **Reduce Open Files in Spark**\n",
        "1. **Consolidate small files**:\n",
        "   ```python\n",
        "   df.repartition(100).write.parquet(\"s3a://bucket/output\")   Fewer partitions = fewer files\n",
        "   ```\n",
        "2. **Tune shuffle partitions**:\n",
        "   ```python\n",
        "   spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")   Default: 200\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Debug Open Files**\n",
        " **Identify Leaky Processes**\n",
        "```bash\n",
        " Find Spark processes\n",
        "ps aux | grep spark\n",
        "\n",
        " Check open files count for a PID\n",
        "ls -l /proc/<PID>/fd | wc -l\n",
        "\n",
        " List open files (Linux)\n",
        "lsof -p <PID> | wc -l\n",
        "```\n",
        "\n",
        " **Check Spark UI**\n",
        "- **Environment Tab**: Verify `spark.files.open` and `spark.redactor.regex`.\n",
        "- **Executors Tab**: Look for executor failures due to `FileNotFoundException`.\n",
        "\n",
        "---\n",
        "\n",
        " **4. Fix Resource Leaks**\n",
        " **In Custom Code**\n",
        "- **Close resources explicitly**:\n",
        "  ```python\n",
        "   Bad: Leaks file handles\n",
        "  with open(\"/tmp/file.txt\", \"w\") as f:\n",
        "      f.write(\"data\")\n",
        "\n",
        "   Good: Explicit close\n",
        "  f = open(\"/tmp/file.txt\", \"w\")\n",
        "  try:\n",
        "      f.write(\"data\")\n",
        "  finally:\n",
        "      f.close()\n",
        "  ```\n",
        "\n",
        " **In Spark Jobs**\n",
        "- **Clean up temporary shuffle files**:\n",
        "  ```python\n",
        "  spark.conf.set(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Configuration Tuning**\n",
        "| **Parameter**                          | **Purpose**                                  | **Recommended Value**       |\n",
        "|----------------------------------------|---------------------------------------------|----------------------------|\n",
        "| `spark.files.maxPartitionBytes`        | Reduce partitions (fewer open files).       | `128MB` (default)          |\n",
        "| `spark.sql.sources.bucketing.enabled`  | Bucketing reduces small files.              | `true`                     |\n",
        "| `spark.shuffle.service.enabled`        | Offloads shuffle files to external service. | `true` (YARN/K8s)          |\n",
        "| `spark.executor.extraJavaOptions`      | Increase JVM file limits.                   | `-XX:MaxDirectMemorySize=1G` |\n",
        "\n",
        "---\n",
        "\n",
        " **6. Advanced: OS-Level Tuning**\n",
        "**Kernel Parameters** (Linux):\n",
        "```bash\n",
        " Increase system-wide file limit\n",
        "echo \"fs.file-max = 100000\" >> /etc/sysctl.conf\n",
        "sysctl -p\n",
        "\n",
        " Increase ephemeral ports (for S3/HDFS connections)\n",
        "echo \"net.ipv4.ip_local_port_range = 10000 65000\" >> /etc/sysctl.conf\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **7. Example Workflow**\n",
        "1. **Check Limits**:\n",
        "   ```bash\n",
        "   ulimit -n\n",
        "   ```\n",
        "2. **Consolidate Files**:\n",
        "   ```python\n",
        "   df = spark.read.parquet(\"s3a://bucket/input/\")\n",
        "   df.coalesce(100).write.parquet(\"s3a://bucket/output/\")\n",
        "   ```\n",
        "3. **Monitor**:\n",
        "   ```bash\n",
        "   watch -n 1 \"lsof -p <Spark_PID> | wc -l\"\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Takeaways**\n",
        "1. **Increase Limits**: Set `ulimit -n 65536` and tune OS settings.\n",
        "2. **Reduce Files**: Consolidate small files, decrease partitions.\n",
        "3. **Fix Leaks**: Audit custom code/UDFs for unclosed resources.\n",
        "4. **Monitor**: Use `lsof` and Spark UI to track file usage.\n",
        "\n",
        "**Pro Tip**: For cloud storage (S3/HDFS), enable **asynchronous I/O** to reduce descriptor pressure:\n",
        "```python\n",
        "spark.conf.set(\"spark.hadoop.fs.s3a.threads.max\", \"20\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "sqY6AlUF9Dk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q19 - Data has duplicates due to appending Parquet without dedup. How to fix?\n",
        "\n",
        "When duplicates arise from **appending to Parquet files without deduplication**, follow this step-by-step approach to clean existing data and prevent future issues:\n",
        "\n",
        "---\n",
        "\n",
        " **1. Identify Duplicates**\n",
        "First, detect duplicates using a **unique key** (e.g., `id`, `timestamp`, or composite columns):\n",
        "```python\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Check duplicate counts\n",
        "dupe_counts = df.groupBy(\"id\", \"timestamp\").agg(count(\"*\").alias(\"count\")) \\\n",
        "                .filter(\"count > 1\")\n",
        "dupe_counts.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **2. Deduplicate Existing Data**\n",
        " **Option A: Overwrite with Distinct Records**\n",
        "```python\n",
        "# Read all data, deduplicate, and overwrite\n",
        "deduped_df = spark.read.parquet(\"s3://your-bucket/data/\") \\\n",
        "                 .dropDuplicates([\"id\", \"timestamp\"])  # Or use primary key(s)\n",
        "\n",
        "deduped_df.write.mode(\"overwrite\").parquet(\"s3://your-bucket/data/\")\n",
        "```\n",
        "\n",
        " **Option B: Use Delta Lake for ACID Transactions**\n",
        "```python\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Convert to Delta Lake and deduplicate\n",
        "delta_table = DeltaTable.convertToDelta(spark, \"parquet.`s3://your-bucket/data/`\")\n",
        "delta_table.alias(\"target\").merge(\n",
        "    delta_table.alias(\"source\"),\n",
        "    \"target.id = source.id AND target.timestamp = source.timestamp\"\n",
        ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **3. Prevent Future Duplicates**\n",
        " **A. Use `ignoreDuplicates` with Delta Lake**\n",
        "```python\n",
        "df.write.format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"ignoreDuplicates\", \"true\") \\\n",
        "    .save(\"s3://your-bucket/data/\")\n",
        "```\n",
        "\n",
        " **B. Pre-Deduplicate Before Append**\n",
        "```python\n",
        "# Deduplicate in-memory before writing\n",
        "df.distinct().write.mode(\"append\").parquet(\"s3://your-bucket/data/\")\n",
        "\n",
        "# Or use window functions for complex logic\n",
        "from pyspark.sql.window import Window\n",
        "window = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").desc())\n",
        "deduped_df = df.withColumn(\"rank\", rank().over(window)) \\\n",
        "               .filter(\"rank = 1\") \\\n",
        "               .drop(\"rank\")\n",
        "```\n",
        "\n",
        " **C. Leverage Partitioning**\n",
        "Partition by a column (e.g., date) to isolate new data before appending:\n",
        "```python\n",
        "df.write.partitionBy(\"date\").mode(\"append\").parquet(\"s3://your-bucket/data/\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Optimize Deduplication Performance**\n",
        "- **Filter New Data First**:  \n",
        "  ```python\n",
        "  new_data = spark.read.parquet(\"s3://new-data/\") \\\n",
        "                   .join(deduped_df, [\"id\", \"timestamp\"], \"left_anti\")  # Only new records\n",
        "  ```\n",
        "- **Use Bloom Filters** (for large datasets):  \n",
        "  ```python\n",
        "  spark.conf.set(\"spark.sql.parquet.bloom.filter.enabled\", \"true\")\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        " **5. Example End-to-End Fix**\n",
        "```python\n",
        "# Step 1: Read all data (including duplicates)\n",
        "df = spark.read.parquet(\"s3://your-bucket/data/\")\n",
        "\n",
        "# Step 2: Deduplicate (keep latest record per ID)\n",
        "from pyspark.sql.window import Window\n",
        "window = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").desc())\n",
        "deduped_df = df.withColumn(\"rank\", rank().over(window)) \\\n",
        "               .filter(\"rank = 1\") \\\n",
        "               .drop(\"rank\")\n",
        "\n",
        "# Step 3: Overwrite with clean data\n",
        "deduped_df.write.mode(\"overwrite\").parquet(\"s3://your-bucket/data/\")\n",
        "\n",
        "# Step 4: Future writes use Delta Lake to prevent dupes\n",
        "deduped_df.write.format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"ignoreDuplicates\", \"true\") \\\n",
        "    .save(\"s3://your-bucket/data/\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Considerations**\n",
        "| **Approach**              | **Pros**                                    | **Cons**                                  |\n",
        "|---------------------------|---------------------------------------------|------------------------------------------|\n",
        "| **`dropDuplicates()`**    | Simple, no new dependencies.                | Slow for large datasets (full scan).     |\n",
        "| **Delta Lake Merge**      | ACID compliant, incremental updates.        | Requires Delta Lake setup.               |\n",
        "| **Pre-filter New Data**   | Fast for incremental loads.                 | Needs business logic for uniqueness.     |\n",
        "\n",
        "**Best Practices**:\n",
        "1. **Add Checksums**: Store hash keys (e.g., `md5(concat(id, timestamp))`) to detect dupes quickly.\n",
        "2. **Monitor**: Log duplicate counts during pipeline runs.\n",
        "3. **Backup**: Snapshot data before overwriting.\n",
        "\n",
        "By combining **immediate cleanup** with **preventive measures**, you ensure data integrity while maintaining pipeline efficiency.\n"
      ],
      "metadata": {
        "id": "6un-G-uk9Gou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " **1. Check Spark UI for Immediate Clues**\n",
        "Access the Spark UI (`http://<driver-node>:4040`) during/after the job and look for:\n",
        "- **Skewed Stages**: Tasks with significantly longer durations than others.\n",
        "  - *Fix*: Use `df.groupBy().count()` to check key distribution; apply salting or repartitioning.\n",
        "- **Garbage Collection (GC) Time**: High GC pauses (>10% of task time).\n",
        "  - *Fix*: Tune JVM settings:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\")\n",
        "    ```\n",
        "- **Shuffle Spill**: Excessive disk spills (`spark.shuffle.spill` metrics).\n",
        "  - *Fix*: Increase executor memory or reduce `spark.sql.shuffle.partitions`.\n",
        "\n",
        "---\n",
        "\n",
        " **2. Review Cluster Metrics**\n",
        "- **Resource Saturation**:\n",
        "  - Check CPU, memory, and network usage on worker nodes (using `htop`, `dstat`, or cloud monitoring tools).\n",
        "  - *Fix*: Allocate more resources or reduce parallelism (`spark.executor.instances`).\n",
        "- **Node Failures**:\n",
        "  - Look for `ExecutorLost` errors in logs.\n",
        "  - *Fix*: Stabilize cluster or use spot instances with fault tolerance.\n",
        "\n",
        "---\n",
        "\n",
        " **3. Identify Data/Code Issues**\n",
        "# **A. Data Skew**\n",
        "- **Diagnose**:\n",
        "  ```python\n",
        "  df.groupBy(\"key\").count().orderBy(\"count\", ascending=False).show()\n",
        "  ```\n",
        "- **Fix**:\n",
        "  - **Salting**: Add random prefixes to skewed keys.\n",
        "    ```python\n",
        "    df = df.withColumn(\"salted_key\", concat(col(\"key\"), lit(\"_\"), (rand() * 10).cast(\"int\")))\n",
        "    ```\n",
        "  - **Repartition**: Balance data manually:\n",
        "    ```python\n",
        "    df = df.repartition(100, \"key\")  # More partitions for skewed keys\n",
        "    ```\n",
        "\n",
        "# **B. Inefficient Queries**\n",
        "- **Diagnose**: Use `df.explain()` to check for:\n",
        "  - Cartesian products.\n",
        "  - Full scans due to missing filters.\n",
        "- **Fix**:\n",
        "  - Push filters upstream:\n",
        "    ```python\n",
        "    df.filter(\"date > '2023-01-01'\").join(...)  # Instead of joining first\n",
        "    ```\n",
        "  - Use broadcast joins for small tables:\n",
        "    ```python\n",
        "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100MB\")\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        " **4. Check External Dependencies**\n",
        "- **Slow Data Sources**: JDBC, S3, or HDFS latency.\n",
        "  - *Fix*:\n",
        "    - For S3: Enable fast upload and multipart:\n",
        "      ```python\n",
        "      spark.conf.set(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
        "      ```\n",
        "    - For JDBC: Increase fetch size:\n",
        "      ```python\n",
        "      df = spark.read.jdbc(url, table, properties={\"fetchSize\": \"5000\"})\n",
        "      ```\n",
        "- **Network Issues**: Latency between executors/driver.\n",
        "  - *Fix*: Use placement groups (AWS/Azure) or monitor network throughput.\n",
        "\n",
        "---\n",
        "\n",
        " **5. Tune Spark Configs**\n",
        "Adjust these key parameters based on workload:\n",
        "| **Config**                              | **Purpose**                                  | **Recommended Value**          |\n",
        "|-----------------------------------------|---------------------------------------------|--------------------------------|\n",
        "| `spark.sql.shuffle.partitions`          | Reduce excessive shuffle partitions.        | `2-4x cores` (e.g., 200)      |\n",
        "| `spark.executor.memoryOverhead`         | Prevent OOMs in off-heap.                  | `1-2GB` (scale with heap)      |\n",
        "| `spark.dynamicAllocation.enabled`       | Scale executors dynamically.               | `true` (for variable loads)    |\n",
        "| `spark.speculation`                     | Mitigate slow tasks.                       | `true` (for spot instances)    |\n",
        "\n",
        "---\n",
        "\n",
        " **6. Log and Profile**\n",
        "- **Enable Detailed Logging**:\n",
        "  ```python\n",
        "  spark.sparkContext.setLogLevel(\"DEBUG\")  # Or \"INFO\" for less noise\n",
        "  ```\n",
        "- **Profile with JVM Tools**:\n",
        "  - Attach `jstack` or `VisualVM` to executors to detect thread contention.\n",
        "\n",
        "---\n",
        "\n",
        " **7. Example Debug Workflow**\n",
        "```python\n",
        "# Step 1: Check skew\n",
        "df.groupBy(\"user_id\").count().orderBy(\"count\").show()\n",
        "\n",
        "# Step 2: Tune shuffle\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "\n",
        "# Step 3: Enable speculation\n",
        "spark.conf.set(\"spark.speculation\", \"true\")\n",
        "\n",
        "# Step 4: Monitor with Spark UI\n",
        "df.join(...).write.parquet(\"output_path\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        " **Key Takeaways**\n",
        "1. **Start with Spark UI**: Identify skewed stages, spills, or GC issues.\n",
        "2. **Verify Cluster Health**: Check CPU/memory/network saturation.\n",
        "3. **Audit Data**: Detect skew or inefficient queries.\n",
        "4. **Tune Configs**: Adjust shuffle, memory, and parallelism.\n",
        "5. **Isolate Variables**: Test with smaller data to reproduce intermittency.\n",
        "\n",
        "**Pro Tip**: For recurring jobs, log metrics (e.g., task duration distributions) to correlate slowness with external factors (e.g., peak cluster usage times).\n"
      ],
      "metadata": {
        "id": "dHZTp26z9RCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nfHyFCGB9zjg"
      }
    }
  ]
}